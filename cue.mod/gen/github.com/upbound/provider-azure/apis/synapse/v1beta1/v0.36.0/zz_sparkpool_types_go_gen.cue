// Code generated by cue get go. DO NOT EDIT.

//cue:generate cue get go github.com/upbound/provider-azure/apis/synapse/v1beta1

package v1beta1

#AutoPauseInitParameters: {
	// Number of minutes of idle time before the Spark Pool is automatically paused. Must be between 5 and 10080.
	delayInMinutes?: null | float64 @go(DelayInMinutes,*float64)
}

#AutoPauseObservation: {
	// Number of minutes of idle time before the Spark Pool is automatically paused. Must be between 5 and 10080.
	delayInMinutes?: null | float64 @go(DelayInMinutes,*float64)
}

#AutoPauseParameters: {
	// Number of minutes of idle time before the Spark Pool is automatically paused. Must be between 5 and 10080.
	// +kubebuilder:validation:Optional
	delayInMinutes?: null | float64 @go(DelayInMinutes,*float64)
}

#AutoScaleInitParameters: {
	// The maximum number of nodes the Spark Pool can support. Must be between 3 and 200.
	maxNodeCount?: null | float64 @go(MaxNodeCount,*float64)

	// The minimum number of nodes the Spark Pool can support. Must be between 3 and 200.
	minNodeCount?: null | float64 @go(MinNodeCount,*float64)
}

#AutoScaleObservation: {
	// The maximum number of nodes the Spark Pool can support. Must be between 3 and 200.
	maxNodeCount?: null | float64 @go(MaxNodeCount,*float64)

	// The minimum number of nodes the Spark Pool can support. Must be between 3 and 200.
	minNodeCount?: null | float64 @go(MinNodeCount,*float64)
}

#AutoScaleParameters: {
	// The maximum number of nodes the Spark Pool can support. Must be between 3 and 200.
	// +kubebuilder:validation:Optional
	maxNodeCount?: null | float64 @go(MaxNodeCount,*float64)

	// The minimum number of nodes the Spark Pool can support. Must be between 3 and 200.
	// +kubebuilder:validation:Optional
	minNodeCount?: null | float64 @go(MinNodeCount,*float64)
}

#LibraryRequirementInitParameters: {
	// The content of library requirements.
	content?: null | string @go(Content,*string)

	// The name of the library requirements file.
	filename?: null | string @go(Filename,*string)
}

#LibraryRequirementObservation: {
	// The content of library requirements.
	content?: null | string @go(Content,*string)

	// The name of the library requirements file.
	filename?: null | string @go(Filename,*string)
}

#LibraryRequirementParameters: {
	// The content of library requirements.
	// +kubebuilder:validation:Optional
	content?: null | string @go(Content,*string)

	// The name of the library requirements file.
	// +kubebuilder:validation:Optional
	filename?: null | string @go(Filename,*string)
}

#SparkConfigInitParameters: {
	// The contents of a spark configuration.
	content?: null | string @go(Content,*string)

	// The name of the file where the spark configuration content will be stored.
	filename?: null | string @go(Filename,*string)
}

#SparkConfigObservation: {
	// The contents of a spark configuration.
	content?: null | string @go(Content,*string)

	// The name of the file where the spark configuration content will be stored.
	filename?: null | string @go(Filename,*string)
}

#SparkConfigParameters: {
	// The contents of a spark configuration.
	// +kubebuilder:validation:Optional
	content?: null | string @go(Content,*string)

	// The name of the file where the spark configuration content will be stored.
	// +kubebuilder:validation:Optional
	filename?: null | string @go(Filename,*string)
}

#SparkPoolInitParameters: {
	// An auto_pause block as defined below.
	autoPause?: [...#AutoPauseInitParameters] @go(AutoPause,[]AutoPauseInitParameters)

	// An auto_scale block as defined below. Exactly one of node_count or auto_scale must be specified.
	autoScale?: [...#AutoScaleInitParameters] @go(AutoScale,[]AutoScaleInitParameters)

	// The cache size in the Spark Pool.
	cacheSize?: null | float64 @go(CacheSize,*float64)

	// Indicates whether compute isolation is enabled or not. Defaults to false.
	computeIsolationEnabled?: null | bool @go(ComputeIsolationEnabled,*bool)

	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to false.
	dynamicExecutorAllocationEnabled?: null | bool @go(DynamicExecutorAllocationEnabled,*bool)

	// A library_requirement block as defined below.
	libraryRequirement?: [...#LibraryRequirementInitParameters] @go(LibraryRequirement,[]LibraryRequirementInitParameters)

	// The maximum number of executors allocated only when dynamic_executor_allocation_enabled set to true.
	maxExecutors?: null | float64 @go(MaxExecutors,*float64)

	// The minimum number of executors allocated only when dynamic_executor_allocation_enabled set to true.
	minExecutors?: null | float64 @go(MinExecutors,*float64)

	// The number of nodes in the Spark Pool. Exactly one of node_count or auto_scale must be specified.
	nodeCount?: null | float64 @go(NodeCount,*float64)

	// The level of node in the Spark Pool. Possible values are Small, Medium, Large, None, XLarge, XXLarge and XXXLarge.
	nodeSize?: null | string @go(NodeSize,*string)

	// The kind of nodes that the Spark Pool provides. Possible values are MemoryOptimized and None.
	nodeSizeFamily?: null | string @go(NodeSizeFamily,*string)

	// Indicates whether session level packages are enabled or not. Defaults to false.
	sessionLevelPackagesEnabled?: null | bool @go(SessionLevelPackagesEnabled,*bool)

	// A spark_config block as defined below.
	sparkConfig?: [...#SparkConfigInitParameters] @go(SparkConfig,[]SparkConfigInitParameters)

	// The Spark events folder. Defaults to /events.
	sparkEventsFolder?: null | string @go(SparkEventsFolder,*string)

	// The default folder where Spark logs will be written. Defaults to /logs.
	sparkLogFolder?: null | string @go(SparkLogFolder,*string)

	// The Apache Spark version. Possible values are 2.4 , 3.1 , 3.2 and 3.3. Defaults to 2.4.
	sparkVersion?: null | string @go(SparkVersion,*string)

	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	tags?: {[string]: null | string} @go(Tags,map[string]*string)
}

#SparkPoolObservation: {
	// An auto_pause block as defined below.
	autoPause?: [...#AutoPauseObservation] @go(AutoPause,[]AutoPauseObservation)

	// An auto_scale block as defined below. Exactly one of node_count or auto_scale must be specified.
	autoScale?: [...#AutoScaleObservation] @go(AutoScale,[]AutoScaleObservation)

	// The cache size in the Spark Pool.
	cacheSize?: null | float64 @go(CacheSize,*float64)

	// Indicates whether compute isolation is enabled or not. Defaults to false.
	computeIsolationEnabled?: null | bool @go(ComputeIsolationEnabled,*bool)

	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to false.
	dynamicExecutorAllocationEnabled?: null | bool @go(DynamicExecutorAllocationEnabled,*bool)

	// The ID of the Synapse Spark Pool.
	id?: null | string @go(ID,*string)

	// A library_requirement block as defined below.
	libraryRequirement?: [...#LibraryRequirementObservation] @go(LibraryRequirement,[]LibraryRequirementObservation)

	// The maximum number of executors allocated only when dynamic_executor_allocation_enabled set to true.
	maxExecutors?: null | float64 @go(MaxExecutors,*float64)

	// The minimum number of executors allocated only when dynamic_executor_allocation_enabled set to true.
	minExecutors?: null | float64 @go(MinExecutors,*float64)

	// The number of nodes in the Spark Pool. Exactly one of node_count or auto_scale must be specified.
	nodeCount?: null | float64 @go(NodeCount,*float64)

	// The level of node in the Spark Pool. Possible values are Small, Medium, Large, None, XLarge, XXLarge and XXXLarge.
	nodeSize?: null | string @go(NodeSize,*string)

	// The kind of nodes that the Spark Pool provides. Possible values are MemoryOptimized and None.
	nodeSizeFamily?: null | string @go(NodeSizeFamily,*string)

	// Indicates whether session level packages are enabled or not. Defaults to false.
	sessionLevelPackagesEnabled?: null | bool @go(SessionLevelPackagesEnabled,*bool)

	// A spark_config block as defined below.
	sparkConfig?: [...#SparkConfigObservation] @go(SparkConfig,[]SparkConfigObservation)

	// The Spark events folder. Defaults to /events.
	sparkEventsFolder?: null | string @go(SparkEventsFolder,*string)

	// The default folder where Spark logs will be written. Defaults to /logs.
	sparkLogFolder?: null | string @go(SparkLogFolder,*string)

	// The Apache Spark version. Possible values are 2.4 , 3.1 , 3.2 and 3.3. Defaults to 2.4.
	sparkVersion?: null | string @go(SparkVersion,*string)

	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	synapseWorkspaceId?: null | string @go(SynapseWorkspaceID,*string)

	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	tags?: {[string]: null | string} @go(Tags,map[string]*string)
}

#SparkPoolParameters: {
	// An auto_pause block as defined below.
	// +kubebuilder:validation:Optional
	autoPause?: [...#AutoPauseParameters] @go(AutoPause,[]AutoPauseParameters)

	// An auto_scale block as defined below. Exactly one of node_count or auto_scale must be specified.
	// +kubebuilder:validation:Optional
	autoScale?: [...#AutoScaleParameters] @go(AutoScale,[]AutoScaleParameters)

	// The cache size in the Spark Pool.
	// +kubebuilder:validation:Optional
	cacheSize?: null | float64 @go(CacheSize,*float64)

	// Indicates whether compute isolation is enabled or not. Defaults to false.
	// +kubebuilder:validation:Optional
	computeIsolationEnabled?: null | bool @go(ComputeIsolationEnabled,*bool)

	// Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to false.
	// +kubebuilder:validation:Optional
	dynamicExecutorAllocationEnabled?: null | bool @go(DynamicExecutorAllocationEnabled,*bool)

	// A library_requirement block as defined below.
	// +kubebuilder:validation:Optional
	libraryRequirement?: [...#LibraryRequirementParameters] @go(LibraryRequirement,[]LibraryRequirementParameters)

	// The maximum number of executors allocated only when dynamic_executor_allocation_enabled set to true.
	// +kubebuilder:validation:Optional
	maxExecutors?: null | float64 @go(MaxExecutors,*float64)

	// The minimum number of executors allocated only when dynamic_executor_allocation_enabled set to true.
	// +kubebuilder:validation:Optional
	minExecutors?: null | float64 @go(MinExecutors,*float64)

	// The number of nodes in the Spark Pool. Exactly one of node_count or auto_scale must be specified.
	// +kubebuilder:validation:Optional
	nodeCount?: null | float64 @go(NodeCount,*float64)

	// The level of node in the Spark Pool. Possible values are Small, Medium, Large, None, XLarge, XXLarge and XXXLarge.
	// +kubebuilder:validation:Optional
	nodeSize?: null | string @go(NodeSize,*string)

	// The kind of nodes that the Spark Pool provides. Possible values are MemoryOptimized and None.
	// +kubebuilder:validation:Optional
	nodeSizeFamily?: null | string @go(NodeSizeFamily,*string)

	// Indicates whether session level packages are enabled or not. Defaults to false.
	// +kubebuilder:validation:Optional
	sessionLevelPackagesEnabled?: null | bool @go(SessionLevelPackagesEnabled,*bool)

	// A spark_config block as defined below.
	// +kubebuilder:validation:Optional
	sparkConfig?: [...#SparkConfigParameters] @go(SparkConfig,[]SparkConfigParameters)

	// The Spark events folder. Defaults to /events.
	// +kubebuilder:validation:Optional
	sparkEventsFolder?: null | string @go(SparkEventsFolder,*string)

	// The default folder where Spark logs will be written. Defaults to /logs.
	// +kubebuilder:validation:Optional
	sparkLogFolder?: null | string @go(SparkLogFolder,*string)

	// The Apache Spark version. Possible values are 2.4 , 3.1 , 3.2 and 3.3. Defaults to 2.4.
	// +kubebuilder:validation:Optional
	sparkVersion?: null | string @go(SparkVersion,*string)

	// The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
	// +crossplane:generate:reference:type=github.com/upbound/provider-azure/apis/synapse/v1beta1.Workspace
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	synapseWorkspaceId?: null | string @go(SynapseWorkspaceID,*string)

	// A mapping of tags which should be assigned to the Synapse Spark Pool.
	// +kubebuilder:validation:Optional
	tags?: {[string]: null | string} @go(Tags,map[string]*string)
}

// SparkPoolSpec defines the desired state of SparkPool
#SparkPoolSpec: {
	forProvider: #SparkPoolParameters @go(ForProvider)

	// THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored
	// unless the relevant Crossplane feature flag is enabled, and may be
	// changed or removed without notice.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	initProvider?: #SparkPoolInitParameters @go(InitProvider)
}

// SparkPoolStatus defines the observed state of SparkPool.
#SparkPoolStatus: {
	atProvider?: #SparkPoolObservation @go(AtProvider)
}

// SparkPool is the Schema for the SparkPools API. Manages a Synapse Spark Pool.
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:subresource:status
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,azure}
#SparkPool: {
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.nodeSize) || has(self.initProvider.nodeSize)",message="nodeSize is a required parameter"
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.nodeSizeFamily) || has(self.initProvider.nodeSizeFamily)",message="nodeSizeFamily is a required parameter"
	spec:    #SparkPoolSpec   @go(Spec)
	status?: #SparkPoolStatus @go(Status)
}

// SparkPoolList contains a list of SparkPools
#SparkPoolList: {
	items: [...#SparkPool] @go(Items,[]SparkPool)
}
