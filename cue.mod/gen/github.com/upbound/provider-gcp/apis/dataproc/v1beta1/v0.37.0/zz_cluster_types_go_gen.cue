// Code generated by cue get go. DO NOT EDIT.

//cue:generate cue get go github.com/upbound/provider-gcp/apis/dataproc/v1beta1

package v1beta1

#AcceleratorsInitParameters: {
	// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
	acceleratorCount?: null | float64 @go(AcceleratorCount,*float64)

	// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
	acceleratorType?: null | string @go(AcceleratorType,*string)
}

#AcceleratorsObservation: {
	// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
	acceleratorCount?: null | float64 @go(AcceleratorCount,*float64)

	// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
	acceleratorType?: null | string @go(AcceleratorType,*string)
}

#AcceleratorsParameters: {
	// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
	// +kubebuilder:validation:Optional
	acceleratorCount?: null | float64 @go(AcceleratorCount,*float64)

	// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
	// +kubebuilder:validation:Optional
	acceleratorType?: null | string @go(AcceleratorType,*string)
}

#AutoscalingConfigInitParameters: {
	// The autoscaling policy used by the cluster.
	policyUri?: null | string @go(PolicyURI,*string)
}

#AutoscalingConfigObservation: {
	// The autoscaling policy used by the cluster.
	policyUri?: null | string @go(PolicyURI,*string)
}

#AutoscalingConfigParameters: {
	// The autoscaling policy used by the cluster.
	// +kubebuilder:validation:Optional
	policyUri?: null | string @go(PolicyURI,*string)
}

#AutoscalingInitParameters: {
	// The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
	maxNodeCount?: null | float64 @go(MaxNodeCount,*float64)

	// The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
	minNodeCount?: null | float64 @go(MinNodeCount,*float64)
}

#AutoscalingObservation: {
	// The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
	maxNodeCount?: null | float64 @go(MaxNodeCount,*float64)

	// The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
	minNodeCount?: null | float64 @go(MinNodeCount,*float64)
}

#AutoscalingParameters: {
	// The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
	// +kubebuilder:validation:Optional
	maxNodeCount?: null | float64 @go(MaxNodeCount,*float64)

	// The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
	// +kubebuilder:validation:Optional
	minNodeCount?: null | float64 @go(MinNodeCount,*float64)
}

#AuxiliaryServicesConfigInitParameters: {
	// The config setting for metastore service with the cluster.
	// Structure defined below.
	metastoreConfig?: [...#AuxiliaryServicesConfigMetastoreConfigInitParameters] @go(MetastoreConfig,[]AuxiliaryServicesConfigMetastoreConfigInitParameters)

	// The Spark History Server configuration for the workload.
	sparkHistoryServerConfig?: [...#SparkHistoryServerConfigInitParameters] @go(SparkHistoryServerConfig,[]SparkHistoryServerConfigInitParameters)
}

#AuxiliaryServicesConfigMetastoreConfigInitParameters: {
	// Resource name of an existing Dataproc Metastore service.
	dataprocMetastoreService?: null | string @go(DataprocMetastoreService,*string)
}

#AuxiliaryServicesConfigMetastoreConfigObservation: {
	// Resource name of an existing Dataproc Metastore service.
	dataprocMetastoreService?: null | string @go(DataprocMetastoreService,*string)
}

#AuxiliaryServicesConfigMetastoreConfigParameters: {
	// Resource name of an existing Dataproc Metastore service.
	// +kubebuilder:validation:Optional
	dataprocMetastoreService?: null | string @go(DataprocMetastoreService,*string)
}

#AuxiliaryServicesConfigObservation: {
	// The config setting for metastore service with the cluster.
	// Structure defined below.
	metastoreConfig?: [...#AuxiliaryServicesConfigMetastoreConfigObservation] @go(MetastoreConfig,[]AuxiliaryServicesConfigMetastoreConfigObservation)

	// The Spark History Server configuration for the workload.
	sparkHistoryServerConfig?: [...#SparkHistoryServerConfigObservation] @go(SparkHistoryServerConfig,[]SparkHistoryServerConfigObservation)
}

#AuxiliaryServicesConfigParameters: {
	// The config setting for metastore service with the cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	metastoreConfig?: [...#AuxiliaryServicesConfigMetastoreConfigParameters] @go(MetastoreConfig,[]AuxiliaryServicesConfigMetastoreConfigParameters)

	// The Spark History Server configuration for the workload.
	// +kubebuilder:validation:Optional
	sparkHistoryServerConfig?: [...#SparkHistoryServerConfigParameters] @go(SparkHistoryServerConfig,[]SparkHistoryServerConfigParameters)
}

#ClusterConfigInitParameters: {
	// The autoscaling policy config associated with the cluster.
	// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
	// only be removed by setting policy_uri = "", rather than removing the whole block.
	// Structure defined below.
	autoscalingConfig?: [...#AutoscalingConfigInitParameters] @go(AutoscalingConfig,[]AutoscalingConfigInitParameters)

	// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
	// Structure defined below.
	dataprocMetricConfig?: [...#DataprocMetricConfigInitParameters] @go(DataprocMetricConfig,[]DataprocMetricConfigInitParameters)

	// The Customer managed encryption keys settings for the cluster.
	// Structure defined below.
	encryptionConfig?: [...#EncryptionConfigInitParameters] @go(EncryptionConfig,[]EncryptionConfigInitParameters)

	// The config settings for port access on the cluster.
	// Structure defined below.
	endpointConfig?: [...#EndpointConfigInitParameters] @go(EndpointConfig,[]EndpointConfigInitParameters)

	// Common config settings for resources of Google Compute Engine cluster
	// instances, applicable to all instances in the cluster. Structure defined below.
	gceClusterConfig?: [...#GceClusterConfigInitParameters] @go(GceClusterConfig,[]GceClusterConfigInitParameters)

	// Commands to execute on each node after config is completed.
	// You can specify multiple versions of these. Structure defined below.
	initializationAction?: [...#InitializationActionInitParameters] @go(InitializationAction,[]InitializationActionInitParameters)

	// The settings for auto deletion cluster schedule.
	// Structure defined below.
	lifecycleConfig?: [...#LifecycleConfigInitParameters] @go(LifecycleConfig,[]LifecycleConfigInitParameters)

	// The Google Compute Engine config settings for the master instances
	// in a cluster. Structure defined below.
	masterConfig?: [...#MasterConfigInitParameters] @go(MasterConfig,[]MasterConfigInitParameters)

	// The config setting for metastore service with the cluster.
	// Structure defined below.
	metastoreConfig?: [...#MetastoreConfigInitParameters] @go(MetastoreConfig,[]MetastoreConfigInitParameters)

	// The Google Compute Engine config settings for the additional
	// instances in a cluster. Structure defined below.
	preemptibleWorkerConfig?: [...#PreemptibleWorkerConfigInitParameters] @go(PreemptibleWorkerConfig,[]PreemptibleWorkerConfigInitParameters)

	// Security related configuration. Structure defined below.
	securityConfig?: [...#SecurityConfigInitParameters] @go(SecurityConfig,[]SecurityConfigInitParameters)

	// The config settings for software inside the cluster.
	// Structure defined below.
	softwareConfig?: [...#SoftwareConfigInitParameters] @go(SoftwareConfig,[]SoftwareConfigInitParameters)

	// The Cloud Storage staging bucket used to stage files,
	// such as Hadoop jars, between client machines and the cluster.
	// Note: If you don't explicitly specify a staging_bucket
	// then GCP will auto create / assign one for you. However, you are not guaranteed
	// an auto generated bucket which is solely dedicated to your cluster; it may be shared
	// with other clusters in the same region/zone also choosing to use the auto generation
	// option.
	stagingBucket?: null | string @go(StagingBucket,*string)

	// The Cloud Storage temp bucket used to store ephemeral cluster
	// and jobs data, such as Spark and MapReduce history files.
	// Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
	tempBucket?: null | string @go(TempBucket,*string)

	// The Google Compute Engine config settings for the worker instances
	// in a cluster. Structure defined below.
	workerConfig?: [...#ClusterConfigWorkerConfigInitParameters] @go(WorkerConfig,[]ClusterConfigWorkerConfigInitParameters)
}

#ClusterConfigObservation: {
	// The autoscaling policy config associated with the cluster.
	// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
	// only be removed by setting policy_uri = "", rather than removing the whole block.
	// Structure defined below.
	autoscalingConfig?: [...#AutoscalingConfigObservation] @go(AutoscalingConfig,[]AutoscalingConfigObservation)

	// The name of the cloud storage bucket ultimately used to house the staging data
	// for the cluster. If staging_bucket is specified, it will contain this value, otherwise
	// it will be the auto generated name.
	bucket?: null | string @go(Bucket,*string)

	// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
	// Structure defined below.
	dataprocMetricConfig?: [...#DataprocMetricConfigObservation] @go(DataprocMetricConfig,[]DataprocMetricConfigObservation)

	// The Customer managed encryption keys settings for the cluster.
	// Structure defined below.
	encryptionConfig?: [...#EncryptionConfigObservation] @go(EncryptionConfig,[]EncryptionConfigObservation)

	// The config settings for port access on the cluster.
	// Structure defined below.
	endpointConfig?: [...#EndpointConfigObservation] @go(EndpointConfig,[]EndpointConfigObservation)

	// Common config settings for resources of Google Compute Engine cluster
	// instances, applicable to all instances in the cluster. Structure defined below.
	gceClusterConfig?: [...#GceClusterConfigObservation] @go(GceClusterConfig,[]GceClusterConfigObservation)

	// Commands to execute on each node after config is completed.
	// You can specify multiple versions of these. Structure defined below.
	initializationAction?: [...#InitializationActionObservation] @go(InitializationAction,[]InitializationActionObservation)

	// The settings for auto deletion cluster schedule.
	// Structure defined below.
	lifecycleConfig?: [...#LifecycleConfigObservation] @go(LifecycleConfig,[]LifecycleConfigObservation)

	// The Google Compute Engine config settings for the master instances
	// in a cluster. Structure defined below.
	masterConfig?: [...#MasterConfigObservation] @go(MasterConfig,[]MasterConfigObservation)

	// The config setting for metastore service with the cluster.
	// Structure defined below.
	metastoreConfig?: [...#MetastoreConfigObservation] @go(MetastoreConfig,[]MetastoreConfigObservation)

	// The Google Compute Engine config settings for the additional
	// instances in a cluster. Structure defined below.
	preemptibleWorkerConfig?: [...#PreemptibleWorkerConfigObservation] @go(PreemptibleWorkerConfig,[]PreemptibleWorkerConfigObservation)

	// Security related configuration. Structure defined below.
	securityConfig?: [...#SecurityConfigObservation] @go(SecurityConfig,[]SecurityConfigObservation)

	// The config settings for software inside the cluster.
	// Structure defined below.
	softwareConfig?: [...#SoftwareConfigObservation] @go(SoftwareConfig,[]SoftwareConfigObservation)

	// The Cloud Storage staging bucket used to stage files,
	// such as Hadoop jars, between client machines and the cluster.
	// Note: If you don't explicitly specify a staging_bucket
	// then GCP will auto create / assign one for you. However, you are not guaranteed
	// an auto generated bucket which is solely dedicated to your cluster; it may be shared
	// with other clusters in the same region/zone also choosing to use the auto generation
	// option.
	stagingBucket?: null | string @go(StagingBucket,*string)

	// The Cloud Storage temp bucket used to store ephemeral cluster
	// and jobs data, such as Spark and MapReduce history files.
	// Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
	tempBucket?: null | string @go(TempBucket,*string)

	// The Google Compute Engine config settings for the worker instances
	// in a cluster. Structure defined below.
	workerConfig?: [...#ClusterConfigWorkerConfigObservation] @go(WorkerConfig,[]ClusterConfigWorkerConfigObservation)
}

#ClusterConfigParameters: {
	// The autoscaling policy config associated with the cluster.
	// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
	// only be removed by setting policy_uri = "", rather than removing the whole block.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	autoscalingConfig?: [...#AutoscalingConfigParameters] @go(AutoscalingConfig,[]AutoscalingConfigParameters)

	// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	dataprocMetricConfig?: [...#DataprocMetricConfigParameters] @go(DataprocMetricConfig,[]DataprocMetricConfigParameters)

	// The Customer managed encryption keys settings for the cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	encryptionConfig?: [...#EncryptionConfigParameters] @go(EncryptionConfig,[]EncryptionConfigParameters)

	// The config settings for port access on the cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	endpointConfig?: [...#EndpointConfigParameters] @go(EndpointConfig,[]EndpointConfigParameters)

	// Common config settings for resources of Google Compute Engine cluster
	// instances, applicable to all instances in the cluster. Structure defined below.
	// +kubebuilder:validation:Optional
	gceClusterConfig?: [...#GceClusterConfigParameters] @go(GceClusterConfig,[]GceClusterConfigParameters)

	// Commands to execute on each node after config is completed.
	// You can specify multiple versions of these. Structure defined below.
	// +kubebuilder:validation:Optional
	initializationAction?: [...#InitializationActionParameters] @go(InitializationAction,[]InitializationActionParameters)

	// The settings for auto deletion cluster schedule.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	lifecycleConfig?: [...#LifecycleConfigParameters] @go(LifecycleConfig,[]LifecycleConfigParameters)

	// The Google Compute Engine config settings for the master instances
	// in a cluster. Structure defined below.
	// +kubebuilder:validation:Optional
	masterConfig?: [...#MasterConfigParameters] @go(MasterConfig,[]MasterConfigParameters)

	// The config setting for metastore service with the cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	metastoreConfig?: [...#MetastoreConfigParameters] @go(MetastoreConfig,[]MetastoreConfigParameters)

	// The Google Compute Engine config settings for the additional
	// instances in a cluster. Structure defined below.
	// +kubebuilder:validation:Optional
	preemptibleWorkerConfig?: [...#PreemptibleWorkerConfigParameters] @go(PreemptibleWorkerConfig,[]PreemptibleWorkerConfigParameters)

	// Security related configuration. Structure defined below.
	// +kubebuilder:validation:Optional
	securityConfig?: [...#SecurityConfigParameters] @go(SecurityConfig,[]SecurityConfigParameters)

	// The config settings for software inside the cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	softwareConfig?: [...#SoftwareConfigParameters] @go(SoftwareConfig,[]SoftwareConfigParameters)

	// The Cloud Storage staging bucket used to stage files,
	// such as Hadoop jars, between client machines and the cluster.
	// Note: If you don't explicitly specify a staging_bucket
	// then GCP will auto create / assign one for you. However, you are not guaranteed
	// an auto generated bucket which is solely dedicated to your cluster; it may be shared
	// with other clusters in the same region/zone also choosing to use the auto generation
	// option.
	// +kubebuilder:validation:Optional
	stagingBucket?: null | string @go(StagingBucket,*string)

	// The Cloud Storage temp bucket used to store ephemeral cluster
	// and jobs data, such as Spark and MapReduce history files.
	// Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
	// +kubebuilder:validation:Optional
	tempBucket?: null | string @go(TempBucket,*string)

	// The Google Compute Engine config settings for the worker instances
	// in a cluster. Structure defined below.
	// +kubebuilder:validation:Optional
	workerConfig?: [...#ClusterConfigWorkerConfigParameters] @go(WorkerConfig,[]ClusterConfigWorkerConfigParameters)
}

#ClusterConfigWorkerConfigInitParameters: {
	// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
	accelerators?: [...#WorkerConfigAcceleratorsInitParameters] @go(Accelerators,[]WorkerConfigAcceleratorsInitParameters)

	// Disk Config
	diskConfig?: [...#WorkerConfigDiskConfigInitParameters] @go(DiskConfig,[]WorkerConfigDiskConfigInitParameters)

	// The URI for the image to use for this worker.  See the guide
	// for more information.
	imageUri?: null | string @go(ImageURI,*string)

	// The name of a Google Compute Engine machine type
	// to create for the worker nodes. If not specified, GCP will default to a predetermined
	// computed value (currently n1-standard-4).
	machineType?: null | string @go(MachineType,*string)

	// The name of a minimum generation of CPU family
	// for the master. If not specified, GCP will default to a predetermined computed value
	// for each zone. See the guide
	// for details about which CPU families are available (and defaulted) for each zone.
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Specifies the number of worker nodes to create.
	// If not specified, GCP will default to a predetermined computed value (currently 2).
	// There is currently a beta feature which allows you to run a
	// Single Node Cluster.
	// In order to take advantage of this you need to set
	// "dataproc:dataproc.allow.zero.workers" = "true" in
	// cluster_config.software_config.properties
	numInstances?: null | float64 @go(NumInstances,*float64)
}

#ClusterConfigWorkerConfigObservation: {
	// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
	accelerators?: [...#WorkerConfigAcceleratorsObservation] @go(Accelerators,[]WorkerConfigAcceleratorsObservation)

	// Disk Config
	diskConfig?: [...#WorkerConfigDiskConfigObservation] @go(DiskConfig,[]WorkerConfigDiskConfigObservation)

	// The URI for the image to use for this worker.  See the guide
	// for more information.
	imageUri?: null | string @go(ImageURI,*string)

	// List of worker instance names which have been assigned
	// to the cluster.
	instanceNames?: [...null | string] @go(InstanceNames,[]*string)

	// The name of a Google Compute Engine machine type
	// to create for the worker nodes. If not specified, GCP will default to a predetermined
	// computed value (currently n1-standard-4).
	machineType?: null | string @go(MachineType,*string)

	// The name of a minimum generation of CPU family
	// for the master. If not specified, GCP will default to a predetermined computed value
	// for each zone. See the guide
	// for details about which CPU families are available (and defaulted) for each zone.
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Specifies the number of worker nodes to create.
	// If not specified, GCP will default to a predetermined computed value (currently 2).
	// There is currently a beta feature which allows you to run a
	// Single Node Cluster.
	// In order to take advantage of this you need to set
	// "dataproc:dataproc.allow.zero.workers" = "true" in
	// cluster_config.software_config.properties
	numInstances?: null | float64 @go(NumInstances,*float64)
}

#ClusterConfigWorkerConfigParameters: {
	// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
	// +kubebuilder:validation:Optional
	accelerators?: [...#WorkerConfigAcceleratorsParameters] @go(Accelerators,[]WorkerConfigAcceleratorsParameters)

	// Disk Config
	// +kubebuilder:validation:Optional
	diskConfig?: [...#WorkerConfigDiskConfigParameters] @go(DiskConfig,[]WorkerConfigDiskConfigParameters)

	// The URI for the image to use for this worker.  See the guide
	// for more information.
	// +kubebuilder:validation:Optional
	imageUri?: null | string @go(ImageURI,*string)

	// The name of a Google Compute Engine machine type
	// to create for the worker nodes. If not specified, GCP will default to a predetermined
	// computed value (currently n1-standard-4).
	// +kubebuilder:validation:Optional
	machineType?: null | string @go(MachineType,*string)

	// The name of a minimum generation of CPU family
	// for the master. If not specified, GCP will default to a predetermined computed value
	// for each zone. See the guide
	// for details about which CPU families are available (and defaulted) for each zone.
	// +kubebuilder:validation:Optional
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Specifies the number of worker nodes to create.
	// If not specified, GCP will default to a predetermined computed value (currently 2).
	// There is currently a beta feature which allows you to run a
	// Single Node Cluster.
	// In order to take advantage of this you need to set
	// "dataproc:dataproc.allow.zero.workers" = "true" in
	// cluster_config.software_config.properties
	// +kubebuilder:validation:Optional
	numInstances?: null | float64 @go(NumInstances,*float64)
}

#ClusterInitParameters: {
	// Allows you to configure various aspects of the cluster.
	// Structure defined below.
	clusterConfig?: [...#ClusterConfigInitParameters] @go(ClusterConfig,[]ClusterConfigInitParameters)

	// Does not affect auto scaling decomissioning from an autoscaling policy.
	// Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.
	// Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).
	// Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of
	// Duration).
	// Only supported on Dataproc image versions 1.2 and higher.
	// For more context see the docs
	gracefulDecommissionTimeout?: null | string @go(GracefulDecommissionTimeout,*string)

	// The list of labels (key/value pairs) to be applied to
	// instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name
	// which is the name of the cluster.
	labels?: {[string]: null | string} @go(Labels,map[string]*string)

	// The name of the cluster, unique within the project and
	// zone.
	name?: null | string @go(Name,*string)

	// The ID of the project in which the cluster will exist. If it
	// is not provided, the provider project is used.
	project?: null | string @go(Project,*string)

	// The region in which the cluster and associated nodes will be created in.
	// Defaults to global.
	region?: null | string @go(Region,*string)

	// Allows you to configure a virtual Dataproc on GKE cluster.
	// Structure defined below.
	virtualClusterConfig?: [...#VirtualClusterConfigInitParameters] @go(VirtualClusterConfig,[]VirtualClusterConfigInitParameters)
}

#ClusterObservation: {
	// Allows you to configure various aspects of the cluster.
	// Structure defined below.
	clusterConfig?: [...#ClusterConfigObservation] @go(ClusterConfig,[]ClusterConfigObservation)

	// Does not affect auto scaling decomissioning from an autoscaling policy.
	// Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.
	// Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).
	// Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of
	// Duration).
	// Only supported on Dataproc image versions 1.2 and higher.
	// For more context see the docs
	gracefulDecommissionTimeout?: null | string @go(GracefulDecommissionTimeout,*string)
	id?:                          null | string @go(ID,*string)

	// The list of labels (key/value pairs) to be applied to
	// instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name
	// which is the name of the cluster.
	labels?: {[string]: null | string} @go(Labels,map[string]*string)

	// The name of the cluster, unique within the project and
	// zone.
	name?: null | string @go(Name,*string)

	// The ID of the project in which the cluster will exist. If it
	// is not provided, the provider project is used.
	project?: null | string @go(Project,*string)

	// The region in which the cluster and associated nodes will be created in.
	// Defaults to global.
	region?: null | string @go(Region,*string)

	// Allows you to configure a virtual Dataproc on GKE cluster.
	// Structure defined below.
	virtualClusterConfig?: [...#VirtualClusterConfigObservation] @go(VirtualClusterConfig,[]VirtualClusterConfigObservation)
}

#ClusterParameters: {
	// Allows you to configure various aspects of the cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	clusterConfig?: [...#ClusterConfigParameters] @go(ClusterConfig,[]ClusterConfigParameters)

	// Does not affect auto scaling decomissioning from an autoscaling policy.
	// Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.
	// Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).
	// Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of
	// Duration).
	// Only supported on Dataproc image versions 1.2 and higher.
	// For more context see the docs
	// +kubebuilder:validation:Optional
	gracefulDecommissionTimeout?: null | string @go(GracefulDecommissionTimeout,*string)

	// The list of labels (key/value pairs) to be applied to
	// instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name
	// which is the name of the cluster.
	// +kubebuilder:validation:Optional
	labels?: {[string]: null | string} @go(Labels,map[string]*string)

	// The name of the cluster, unique within the project and
	// zone.
	// +kubebuilder:validation:Optional
	name?: null | string @go(Name,*string)

	// The ID of the project in which the cluster will exist. If it
	// is not provided, the provider project is used.
	// +kubebuilder:validation:Optional
	project?: null | string @go(Project,*string)

	// The region in which the cluster and associated nodes will be created in.
	// Defaults to global.
	// +kubebuilder:validation:Optional
	region?: null | string @go(Region,*string)

	// Allows you to configure a virtual Dataproc on GKE cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	virtualClusterConfig?: [...#VirtualClusterConfigParameters] @go(VirtualClusterConfig,[]VirtualClusterConfigParameters)
}

#ConfigInitParameters: {
	// The number of local SSD disks to attach to the node,
	// which is limited by the maximum number of disks allowable per zone.
	localSsdCount?: null | float64 @go(LocalSsdCount,*float64)

	// The name of a Compute Engine machine type.
	machineType?: null | string @go(MachineType,*string)

	// Minimum CPU platform to be used by this instance.
	// The instance may be scheduled on the specified or a newer CPU platform.
	// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Whether the nodes are created as preemptible VM instances.
	// Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
	// CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	preemptible?: null | bool @go(Preemptible,*bool)

	// Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
	spot?: null | bool @go(Spot,*bool)
}

#ConfigObservation: {
	// The number of local SSD disks to attach to the node,
	// which is limited by the maximum number of disks allowable per zone.
	localSsdCount?: null | float64 @go(LocalSsdCount,*float64)

	// The name of a Compute Engine machine type.
	machineType?: null | string @go(MachineType,*string)

	// Minimum CPU platform to be used by this instance.
	// The instance may be scheduled on the specified or a newer CPU platform.
	// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Whether the nodes are created as preemptible VM instances.
	// Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
	// CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	preemptible?: null | bool @go(Preemptible,*bool)

	// Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
	spot?: null | bool @go(Spot,*bool)
}

#ConfigParameters: {
	// The number of local SSD disks to attach to the node,
	// which is limited by the maximum number of disks allowable per zone.
	// +kubebuilder:validation:Optional
	localSsdCount?: null | float64 @go(LocalSsdCount,*float64)

	// The name of a Compute Engine machine type.
	// +kubebuilder:validation:Optional
	machineType?: null | string @go(MachineType,*string)

	// Minimum CPU platform to be used by this instance.
	// The instance may be scheduled on the specified or a newer CPU platform.
	// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
	// +kubebuilder:validation:Optional
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Whether the nodes are created as preemptible VM instances.
	// Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
	// CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	// +kubebuilder:validation:Optional
	preemptible?: null | bool @go(Preemptible,*bool)

	// Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
	// +kubebuilder:validation:Optional
	spot?: null | bool @go(Spot,*bool)
}

#DataprocMetricConfigInitParameters: {
	// Metrics sources to enable.
	metrics?: [...#MetricsInitParameters] @go(Metrics,[]MetricsInitParameters)
}

#DataprocMetricConfigObservation: {
	// Metrics sources to enable.
	metrics?: [...#MetricsObservation] @go(Metrics,[]MetricsObservation)
}

#DataprocMetricConfigParameters: {
	// Metrics sources to enable.
	// +kubebuilder:validation:Optional
	metrics: [...#MetricsParameters] @go(Metrics,[]MetricsParameters)
}

#DiskConfigInitParameters: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#DiskConfigObservation: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#DiskConfigParameters: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	// +kubebuilder:validation:Optional
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	// +kubebuilder:validation:Optional
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	// +kubebuilder:validation:Optional
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#EncryptionConfigInitParameters: {
	// The Cloud KMS key name to use for PD disk encryption for
	// all instances in the cluster.
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#EncryptionConfigObservation: {
	// The Cloud KMS key name to use for PD disk encryption for
	// all instances in the cluster.
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#EncryptionConfigParameters: {
	// The Cloud KMS key name to use for PD disk encryption for
	// all instances in the cluster.
	// +kubebuilder:validation:Optional
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#EndpointConfigInitParameters: {
	// The flag to enable http access to specific ports
	// on the cluster from external sources (aka Component Gateway). Defaults to false.
	enableHttpPortAccess?: null | bool @go(EnableHTTPPortAccess,*bool)
}

#EndpointConfigObservation: {
	// The flag to enable http access to specific ports
	// on the cluster from external sources (aka Component Gateway). Defaults to false.
	enableHttpPortAccess?: null | bool @go(EnableHTTPPortAccess,*bool)

	// The map of port descriptions to URLs. Will only be populated if
	// enable_http_port_access is true.
	httpPorts?: {[string]: null | string} @go(HTTPPorts,map[string]*string)
}

#EndpointConfigParameters: {
	// The flag to enable http access to specific ports
	// on the cluster from external sources (aka Component Gateway). Defaults to false.
	// +kubebuilder:validation:Optional
	enableHttpPortAccess?: null | bool @go(EnableHTTPPortAccess,*bool)
}

#GceClusterConfigInitParameters: {
	// By default, clusters are not restricted to internal IP addresses,
	// and will have ephemeral external IP addresses assigned to each instance. If set to true, all
	// instances in the cluster will only have internal IP addresses. Note: Private Google Access
	// (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster
	// will be launched in.
	internalIpOnly?: null | bool @go(InternalIPOnly,*bool)

	// A map of the Compute Engine metadata entries to add to all instances
	// (see Project and instance metadata).
	metadata?: {[string]: null | string} @go(Metadata,map[string]*string)

	// The name or self_link of the Google Compute Engine
	// network to the cluster will be part of. Conflicts with subnetwork.
	// If neither is specified, this defaults to the "default" network.
	network?: null | string @go(Network,*string)

	// Node Group Affinity for sole-tenant clusters.
	nodeGroupAffinity?: [...#NodeGroupAffinityInitParameters] @go(NodeGroupAffinity,[]NodeGroupAffinityInitParameters)

	// Reservation Affinity for consuming zonal reservation.
	reservationAffinity?: [...#ReservationAffinityInitParameters] @go(ReservationAffinity,[]ReservationAffinityInitParameters)

	// The set of Google API scopes
	// to be made available on all of the node VMs under the service_account
	// specified. Both OAuth2 URLs and gcloud
	// short names are supported. To allow full access to all Cloud APIs, use the
	// cloud-platform scope. See a complete list of scopes here.
	serviceAccountScopes?: [...null | string] @go(ServiceAccountScopes,[]*string)

	// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
	shieldedInstanceConfig?: [...#ShieldedInstanceConfigInitParameters] @go(ShieldedInstanceConfig,[]ShieldedInstanceConfigInitParameters)

	// The name or self_link of the Google Compute Engine
	// subnetwork the cluster will be part of. Conflicts with network.
	subnetwork?: null | string @go(Subnetwork,*string)

	// The list of instance tags applied to instances in the cluster.
	// Tags are used to identify valid sources or targets for network firewalls.
	tags?: [...null | string] @go(Tags,[]*string)

	// The GCP zone where your data is stored and used (i.e. where
	// the master and the worker nodes will be created in). If region is set to 'global' (default)
	// then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement
	// to determine this automatically for you.
	// Note: This setting additionally determines and restricts
	// which computing resources are available for use with other configs such as
	// cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
	zone?: null | string @go(Zone,*string)
}

#GceClusterConfigObservation: {
	// By default, clusters are not restricted to internal IP addresses,
	// and will have ephemeral external IP addresses assigned to each instance. If set to true, all
	// instances in the cluster will only have internal IP addresses. Note: Private Google Access
	// (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster
	// will be launched in.
	internalIpOnly?: null | bool @go(InternalIPOnly,*bool)

	// A map of the Compute Engine metadata entries to add to all instances
	// (see Project and instance metadata).
	metadata?: {[string]: null | string} @go(Metadata,map[string]*string)

	// The name or self_link of the Google Compute Engine
	// network to the cluster will be part of. Conflicts with subnetwork.
	// If neither is specified, this defaults to the "default" network.
	network?: null | string @go(Network,*string)

	// Node Group Affinity for sole-tenant clusters.
	nodeGroupAffinity?: [...#NodeGroupAffinityObservation] @go(NodeGroupAffinity,[]NodeGroupAffinityObservation)

	// Reservation Affinity for consuming zonal reservation.
	reservationAffinity?: [...#ReservationAffinityObservation] @go(ReservationAffinity,[]ReservationAffinityObservation)

	// The service account to be used by the Node VMs.
	// If not specified, the "default" service account is used.
	serviceAccount?: null | string @go(ServiceAccount,*string)

	// The set of Google API scopes
	// to be made available on all of the node VMs under the service_account
	// specified. Both OAuth2 URLs and gcloud
	// short names are supported. To allow full access to all Cloud APIs, use the
	// cloud-platform scope. See a complete list of scopes here.
	serviceAccountScopes?: [...null | string] @go(ServiceAccountScopes,[]*string)

	// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
	shieldedInstanceConfig?: [...#ShieldedInstanceConfigObservation] @go(ShieldedInstanceConfig,[]ShieldedInstanceConfigObservation)

	// The name or self_link of the Google Compute Engine
	// subnetwork the cluster will be part of. Conflicts with network.
	subnetwork?: null | string @go(Subnetwork,*string)

	// The list of instance tags applied to instances in the cluster.
	// Tags are used to identify valid sources or targets for network firewalls.
	tags?: [...null | string] @go(Tags,[]*string)

	// The GCP zone where your data is stored and used (i.e. where
	// the master and the worker nodes will be created in). If region is set to 'global' (default)
	// then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement
	// to determine this automatically for you.
	// Note: This setting additionally determines and restricts
	// which computing resources are available for use with other configs such as
	// cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
	zone?: null | string @go(Zone,*string)
}

#GceClusterConfigParameters: {
	// By default, clusters are not restricted to internal IP addresses,
	// and will have ephemeral external IP addresses assigned to each instance. If set to true, all
	// instances in the cluster will only have internal IP addresses. Note: Private Google Access
	// (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster
	// will be launched in.
	// +kubebuilder:validation:Optional
	internalIpOnly?: null | bool @go(InternalIPOnly,*bool)

	// A map of the Compute Engine metadata entries to add to all instances
	// (see Project and instance metadata).
	// +kubebuilder:validation:Optional
	metadata?: {[string]: null | string} @go(Metadata,map[string]*string)

	// The name or self_link of the Google Compute Engine
	// network to the cluster will be part of. Conflicts with subnetwork.
	// If neither is specified, this defaults to the "default" network.
	// +kubebuilder:validation:Optional
	network?: null | string @go(Network,*string)

	// Node Group Affinity for sole-tenant clusters.
	// +kubebuilder:validation:Optional
	nodeGroupAffinity?: [...#NodeGroupAffinityParameters] @go(NodeGroupAffinity,[]NodeGroupAffinityParameters)

	// Reservation Affinity for consuming zonal reservation.
	// +kubebuilder:validation:Optional
	reservationAffinity?: [...#ReservationAffinityParameters] @go(ReservationAffinity,[]ReservationAffinityParameters)

	// The service account to be used by the Node VMs.
	// If not specified, the "default" service account is used.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/cloudplatform/v1beta1.ServiceAccount
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractParamPath("email",true)
	// +kubebuilder:validation:Optional
	serviceAccount?: null | string @go(ServiceAccount,*string)

	// The set of Google API scopes
	// to be made available on all of the node VMs under the service_account
	// specified. Both OAuth2 URLs and gcloud
	// short names are supported. To allow full access to all Cloud APIs, use the
	// cloud-platform scope. See a complete list of scopes here.
	// +kubebuilder:validation:Optional
	serviceAccountScopes?: [...null | string] @go(ServiceAccountScopes,[]*string)

	// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
	// +kubebuilder:validation:Optional
	shieldedInstanceConfig?: [...#ShieldedInstanceConfigParameters] @go(ShieldedInstanceConfig,[]ShieldedInstanceConfigParameters)

	// The name or self_link of the Google Compute Engine
	// subnetwork the cluster will be part of. Conflicts with network.
	// +kubebuilder:validation:Optional
	subnetwork?: null | string @go(Subnetwork,*string)

	// The list of instance tags applied to instances in the cluster.
	// Tags are used to identify valid sources or targets for network firewalls.
	// +kubebuilder:validation:Optional
	tags?: [...null | string] @go(Tags,[]*string)

	// The GCP zone where your data is stored and used (i.e. where
	// the master and the worker nodes will be created in). If region is set to 'global' (default)
	// then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement
	// to determine this automatically for you.
	// Note: This setting additionally determines and restricts
	// which computing resources are available for use with other configs such as
	// cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
	// +kubebuilder:validation:Optional
	zone?: null | string @go(Zone,*string)
}

#GkeClusterConfigInitParameters: {
	// A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster
	// (the GKE cluster can be zonal or regional)
	gkeClusterTarget?: null | string @go(GkeClusterTarget,*string)

	// GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT
	// GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
	// Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
	nodePoolTarget?: [...#NodePoolTargetInitParameters] @go(NodePoolTarget,[]NodePoolTargetInitParameters)
}

#GkeClusterConfigObservation: {
	// A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster
	// (the GKE cluster can be zonal or regional)
	gkeClusterTarget?: null | string @go(GkeClusterTarget,*string)

	// GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT
	// GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
	// Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
	nodePoolTarget?: [...#NodePoolTargetObservation] @go(NodePoolTarget,[]NodePoolTargetObservation)
}

#GkeClusterConfigParameters: {
	// A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster
	// (the GKE cluster can be zonal or regional)
	// +kubebuilder:validation:Optional
	gkeClusterTarget?: null | string @go(GkeClusterTarget,*string)

	// GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT
	// GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
	// Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
	// +kubebuilder:validation:Optional
	nodePoolTarget?: [...#NodePoolTargetParameters] @go(NodePoolTarget,[]NodePoolTargetParameters)
}

#InitializationActionInitParameters: {
	// The script to be executed during initialization of the cluster.
	// The script must be a GCS file with a gs:// prefix.
	script?: null | string @go(Script,*string)

	// The maximum duration (in seconds) which script is
	// allowed to take to execute its action. GCP will default to a predetermined
	// computed value if not set (currently 300).
	timeoutSec?: null | float64 @go(TimeoutSec,*float64)
}

#InitializationActionObservation: {
	// The script to be executed during initialization of the cluster.
	// The script must be a GCS file with a gs:// prefix.
	script?: null | string @go(Script,*string)

	// The maximum duration (in seconds) which script is
	// allowed to take to execute its action. GCP will default to a predetermined
	// computed value if not set (currently 300).
	timeoutSec?: null | float64 @go(TimeoutSec,*float64)
}

#InitializationActionParameters: {
	// The script to be executed during initialization of the cluster.
	// The script must be a GCS file with a gs:// prefix.
	// +kubebuilder:validation:Optional
	script?: null | string @go(Script,*string)

	// The maximum duration (in seconds) which script is
	// allowed to take to execute its action. GCP will default to a predetermined
	// computed value if not set (currently 300).
	// +kubebuilder:validation:Optional
	timeoutSec?: null | float64 @go(TimeoutSec,*float64)
}

#KerberosConfigInitParameters: {
	// The admin server (IP or hostname) for the
	// remote trusted realm in a cross realm trust relationship.
	crossRealmTrustAdminServer?: null | string @go(CrossRealmTrustAdminServer,*string)

	// The KDC (IP or hostname) for the
	// remote trusted realm in a cross realm trust relationship.
	crossRealmTrustKdc?: null | string @go(CrossRealmTrustKdc,*string)

	// The remote realm the Dataproc on-cluster KDC will
	// trust, should the user enable cross realm trust.
	crossRealmTrustRealm?: null | string @go(CrossRealmTrustRealm,*string)

	// The Cloud Storage URI of a KMS
	// encrypted file containing the shared password between the on-cluster Kerberos realm
	// and the remote trusted realm, in a cross realm trust relationship.
	crossRealmTrustSharedPasswordUri?: null | string @go(CrossRealmTrustSharedPasswordURI,*string)

	// Flag to indicate whether to Kerberize the cluster.
	enableKerberos?: null | bool @go(EnableKerberos,*bool)

	// The URI of the KMS key used to encrypt various sensitive files.
	kmsKeyUri?: null | string @go(KMSKeyURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the master key of the KDC database.
	kdcDbKeyUri?: null | string @go(KdcDBKeyURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the password to the user provided key. For the self-signed certificate, this password
	// is generated by Dataproc.
	keyPasswordUri?: null | string @go(KeyPasswordURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the password to the user provided keystore. For the self-signed certificated, the password
	// is generated by Dataproc.
	keystorePasswordUri?: null | string @go(KeystorePasswordURI,*string)

	// The Cloud Storage URI of the keystore file used for SSL encryption.
	// If not provided, Dataproc will provide a self-signed certificate.
	keystoreUri?: null | string @go(KeystoreURI,*string)

	// The name of the on-cluster Kerberos realm. If not specified, the
	// uppercased domain of hostnames will be the realm.
	realm?: null | string @go(Realm,*string)

	// The Cloud Storage URI of a KMS encrypted file
	// containing the root principal password.
	rootPrincipalPasswordUri?: null | string @go(RootPrincipalPasswordURI,*string)

	// The lifetime of the ticket granting ticket, in hours.
	tgtLifetimeHours?: null | float64 @go(TgtLifetimeHours,*float64)

	// The Cloud Storage URI of a KMS encrypted file
	// containing the password to the user provided truststore. For the self-signed
	// certificate, this password is generated by Dataproc.
	truststorePasswordUri?: null | string @go(TruststorePasswordURI,*string)

	// The Cloud Storage URI of the truststore file used for
	// SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	truststoreUri?: null | string @go(TruststoreURI,*string)
}

#KerberosConfigObservation: {
	// The admin server (IP or hostname) for the
	// remote trusted realm in a cross realm trust relationship.
	crossRealmTrustAdminServer?: null | string @go(CrossRealmTrustAdminServer,*string)

	// The KDC (IP or hostname) for the
	// remote trusted realm in a cross realm trust relationship.
	crossRealmTrustKdc?: null | string @go(CrossRealmTrustKdc,*string)

	// The remote realm the Dataproc on-cluster KDC will
	// trust, should the user enable cross realm trust.
	crossRealmTrustRealm?: null | string @go(CrossRealmTrustRealm,*string)

	// The Cloud Storage URI of a KMS
	// encrypted file containing the shared password between the on-cluster Kerberos realm
	// and the remote trusted realm, in a cross realm trust relationship.
	crossRealmTrustSharedPasswordUri?: null | string @go(CrossRealmTrustSharedPasswordURI,*string)

	// Flag to indicate whether to Kerberize the cluster.
	enableKerberos?: null | bool @go(EnableKerberos,*bool)

	// The URI of the KMS key used to encrypt various sensitive files.
	kmsKeyUri?: null | string @go(KMSKeyURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the master key of the KDC database.
	kdcDbKeyUri?: null | string @go(KdcDBKeyURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the password to the user provided key. For the self-signed certificate, this password
	// is generated by Dataproc.
	keyPasswordUri?: null | string @go(KeyPasswordURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the password to the user provided keystore. For the self-signed certificated, the password
	// is generated by Dataproc.
	keystorePasswordUri?: null | string @go(KeystorePasswordURI,*string)

	// The Cloud Storage URI of the keystore file used for SSL encryption.
	// If not provided, Dataproc will provide a self-signed certificate.
	keystoreUri?: null | string @go(KeystoreURI,*string)

	// The name of the on-cluster Kerberos realm. If not specified, the
	// uppercased domain of hostnames will be the realm.
	realm?: null | string @go(Realm,*string)

	// The Cloud Storage URI of a KMS encrypted file
	// containing the root principal password.
	rootPrincipalPasswordUri?: null | string @go(RootPrincipalPasswordURI,*string)

	// The lifetime of the ticket granting ticket, in hours.
	tgtLifetimeHours?: null | float64 @go(TgtLifetimeHours,*float64)

	// The Cloud Storage URI of a KMS encrypted file
	// containing the password to the user provided truststore. For the self-signed
	// certificate, this password is generated by Dataproc.
	truststorePasswordUri?: null | string @go(TruststorePasswordURI,*string)

	// The Cloud Storage URI of the truststore file used for
	// SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	truststoreUri?: null | string @go(TruststoreURI,*string)
}

#KerberosConfigParameters: {
	// The admin server (IP or hostname) for the
	// remote trusted realm in a cross realm trust relationship.
	// +kubebuilder:validation:Optional
	crossRealmTrustAdminServer?: null | string @go(CrossRealmTrustAdminServer,*string)

	// The KDC (IP or hostname) for the
	// remote trusted realm in a cross realm trust relationship.
	// +kubebuilder:validation:Optional
	crossRealmTrustKdc?: null | string @go(CrossRealmTrustKdc,*string)

	// The remote realm the Dataproc on-cluster KDC will
	// trust, should the user enable cross realm trust.
	// +kubebuilder:validation:Optional
	crossRealmTrustRealm?: null | string @go(CrossRealmTrustRealm,*string)

	// The Cloud Storage URI of a KMS
	// encrypted file containing the shared password between the on-cluster Kerberos realm
	// and the remote trusted realm, in a cross realm trust relationship.
	// +kubebuilder:validation:Optional
	crossRealmTrustSharedPasswordUri?: null | string @go(CrossRealmTrustSharedPasswordURI,*string)

	// Flag to indicate whether to Kerberize the cluster.
	// +kubebuilder:validation:Optional
	enableKerberos?: null | bool @go(EnableKerberos,*bool)

	// The URI of the KMS key used to encrypt various sensitive files.
	// +kubebuilder:validation:Optional
	kmsKeyUri?: null | string @go(KMSKeyURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the master key of the KDC database.
	// +kubebuilder:validation:Optional
	kdcDbKeyUri?: null | string @go(KdcDBKeyURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the password to the user provided key. For the self-signed certificate, this password
	// is generated by Dataproc.
	// +kubebuilder:validation:Optional
	keyPasswordUri?: null | string @go(KeyPasswordURI,*string)

	// The Cloud Storage URI of a KMS encrypted file containing
	// the password to the user provided keystore. For the self-signed certificated, the password
	// is generated by Dataproc.
	// +kubebuilder:validation:Optional
	keystorePasswordUri?: null | string @go(KeystorePasswordURI,*string)

	// The Cloud Storage URI of the keystore file used for SSL encryption.
	// If not provided, Dataproc will provide a self-signed certificate.
	// +kubebuilder:validation:Optional
	keystoreUri?: null | string @go(KeystoreURI,*string)

	// The name of the on-cluster Kerberos realm. If not specified, the
	// uppercased domain of hostnames will be the realm.
	// +kubebuilder:validation:Optional
	realm?: null | string @go(Realm,*string)

	// The Cloud Storage URI of a KMS encrypted file
	// containing the root principal password.
	// +kubebuilder:validation:Optional
	rootPrincipalPasswordUri?: null | string @go(RootPrincipalPasswordURI,*string)

	// The lifetime of the ticket granting ticket, in hours.
	// +kubebuilder:validation:Optional
	tgtLifetimeHours?: null | float64 @go(TgtLifetimeHours,*float64)

	// The Cloud Storage URI of a KMS encrypted file
	// containing the password to the user provided truststore. For the self-signed
	// certificate, this password is generated by Dataproc.
	// +kubebuilder:validation:Optional
	truststorePasswordUri?: null | string @go(TruststorePasswordURI,*string)

	// The Cloud Storage URI of the truststore file used for
	// SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	// +kubebuilder:validation:Optional
	truststoreUri?: null | string @go(TruststoreURI,*string)
}

#KubernetesClusterConfigInitParameters: {
	// The configuration for running the Dataproc cluster on GKE.
	gkeClusterConfig?: [...#GkeClusterConfigInitParameters] @go(GkeClusterConfig,[]GkeClusterConfigInitParameters)

	// A namespace within the Kubernetes cluster to deploy into.
	// If this namespace does not exist, it is created.
	// If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
	// If not specified, the name of the Dataproc Cluster is used.
	kubernetesNamespace?: null | string @go(KubernetesNamespace,*string)

	// The software configuration for this Dataproc cluster running on Kubernetes.
	kubernetesSoftwareConfig?: [...#KubernetesSoftwareConfigInitParameters] @go(KubernetesSoftwareConfig,[]KubernetesSoftwareConfigInitParameters)
}

#KubernetesClusterConfigObservation: {
	// The configuration for running the Dataproc cluster on GKE.
	gkeClusterConfig?: [...#GkeClusterConfigObservation] @go(GkeClusterConfig,[]GkeClusterConfigObservation)

	// A namespace within the Kubernetes cluster to deploy into.
	// If this namespace does not exist, it is created.
	// If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
	// If not specified, the name of the Dataproc Cluster is used.
	kubernetesNamespace?: null | string @go(KubernetesNamespace,*string)

	// The software configuration for this Dataproc cluster running on Kubernetes.
	kubernetesSoftwareConfig?: [...#KubernetesSoftwareConfigObservation] @go(KubernetesSoftwareConfig,[]KubernetesSoftwareConfigObservation)
}

#KubernetesClusterConfigParameters: {
	// The configuration for running the Dataproc cluster on GKE.
	// +kubebuilder:validation:Optional
	gkeClusterConfig: [...#GkeClusterConfigParameters] @go(GkeClusterConfig,[]GkeClusterConfigParameters)

	// A namespace within the Kubernetes cluster to deploy into.
	// If this namespace does not exist, it is created.
	// If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
	// If not specified, the name of the Dataproc Cluster is used.
	// +kubebuilder:validation:Optional
	kubernetesNamespace?: null | string @go(KubernetesNamespace,*string)

	// The software configuration for this Dataproc cluster running on Kubernetes.
	// +kubebuilder:validation:Optional
	kubernetesSoftwareConfig: [...#KubernetesSoftwareConfigParameters] @go(KubernetesSoftwareConfig,[]KubernetesSoftwareConfigParameters)
}

#KubernetesSoftwareConfigInitParameters: {
	// The components that should be installed in this Dataproc cluster. The key must be a string from the
	// KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
	componentVersion?: {[string]: null | string} @go(ComponentVersion,map[string]*string)

	// The properties to set on daemon config files. Property keys are specified in prefix:property format,
	// for example spark:spark.kubernetes.container.image.
	properties?: {[string]: null | string} @go(Properties,map[string]*string)
}

#KubernetesSoftwareConfigObservation: {
	// The components that should be installed in this Dataproc cluster. The key must be a string from the
	// KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
	componentVersion?: {[string]: null | string} @go(ComponentVersion,map[string]*string)

	// The properties to set on daemon config files. Property keys are specified in prefix:property format,
	// for example spark:spark.kubernetes.container.image.
	properties?: {[string]: null | string} @go(Properties,map[string]*string)
}

#KubernetesSoftwareConfigParameters: {
	// The components that should be installed in this Dataproc cluster. The key must be a string from the
	// KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
	// +kubebuilder:validation:Optional
	componentVersion: {[string]: null | string} @go(ComponentVersion,map[string]*string)

	// The properties to set on daemon config files. Property keys are specified in prefix:property format,
	// for example spark:spark.kubernetes.container.image.
	// +kubebuilder:validation:Optional
	properties?: {[string]: null | string} @go(Properties,map[string]*string)
}

#LifecycleConfigInitParameters: {
	// The time when cluster will be auto-deleted.
	// A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
	// Example: "2014-10-02T15:01:23.045123456Z".
	autoDeleteTime?: null | string @go(AutoDeleteTime,*string)

	// The duration to keep the cluster alive while idling
	// (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
	idleDeleteTtl?: null | string @go(IdleDeleteTTL,*string)
}

#LifecycleConfigObservation: {
	// The time when cluster will be auto-deleted.
	// A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
	// Example: "2014-10-02T15:01:23.045123456Z".
	autoDeleteTime?: null | string @go(AutoDeleteTime,*string)

	// The duration to keep the cluster alive while idling
	// (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
	idleDeleteTtl?: null | string @go(IdleDeleteTTL,*string)

	// Time when the cluster became idle
	// (most recent job finished) and became eligible for deletion due to idleness.
	idleStartTime?: null | string @go(IdleStartTime,*string)
}

#LifecycleConfigParameters: {
	// The time when cluster will be auto-deleted.
	// A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
	// Example: "2014-10-02T15:01:23.045123456Z".
	// +kubebuilder:validation:Optional
	autoDeleteTime?: null | string @go(AutoDeleteTime,*string)

	// The duration to keep the cluster alive while idling
	// (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
	// +kubebuilder:validation:Optional
	idleDeleteTtl?: null | string @go(IdleDeleteTTL,*string)
}

#MasterConfigInitParameters: {
	// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
	accelerators?: [...#AcceleratorsInitParameters] @go(Accelerators,[]AcceleratorsInitParameters)

	// Disk Config
	diskConfig?: [...#DiskConfigInitParameters] @go(DiskConfig,[]DiskConfigInitParameters)

	// The URI for the image to use for this worker.  See the guide
	// for more information.
	imageUri?: null | string @go(ImageURI,*string)

	// The name of a Google Compute Engine machine type
	// to create for the master. If not specified, GCP will default to a predetermined
	// computed value (currently n1-standard-4).
	machineType?: null | string @go(MachineType,*string)

	// The name of a minimum generation of CPU family
	// for the master. If not specified, GCP will default to a predetermined computed value
	// for each zone. See the guide
	// for details about which CPU families are available (and defaulted) for each zone.
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Specifies the number of master nodes to create.
	// If not specified, GCP will default to a predetermined computed value (currently 1).
	numInstances?: null | float64 @go(NumInstances,*float64)
}

#MasterConfigObservation: {
	// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
	accelerators?: [...#AcceleratorsObservation] @go(Accelerators,[]AcceleratorsObservation)

	// Disk Config
	diskConfig?: [...#DiskConfigObservation] @go(DiskConfig,[]DiskConfigObservation)

	// The URI for the image to use for this worker.  See the guide
	// for more information.
	imageUri?: null | string @go(ImageURI,*string)

	// List of worker instance names which have been assigned
	// to the cluster.
	instanceNames?: [...null | string] @go(InstanceNames,[]*string)

	// The name of a Google Compute Engine machine type
	// to create for the master. If not specified, GCP will default to a predetermined
	// computed value (currently n1-standard-4).
	machineType?: null | string @go(MachineType,*string)

	// The name of a minimum generation of CPU family
	// for the master. If not specified, GCP will default to a predetermined computed value
	// for each zone. See the guide
	// for details about which CPU families are available (and defaulted) for each zone.
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Specifies the number of master nodes to create.
	// If not specified, GCP will default to a predetermined computed value (currently 1).
	numInstances?: null | float64 @go(NumInstances,*float64)
}

#MasterConfigParameters: {
	// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
	// +kubebuilder:validation:Optional
	accelerators?: [...#AcceleratorsParameters] @go(Accelerators,[]AcceleratorsParameters)

	// Disk Config
	// +kubebuilder:validation:Optional
	diskConfig?: [...#DiskConfigParameters] @go(DiskConfig,[]DiskConfigParameters)

	// The URI for the image to use for this worker.  See the guide
	// for more information.
	// +kubebuilder:validation:Optional
	imageUri?: null | string @go(ImageURI,*string)

	// The name of a Google Compute Engine machine type
	// to create for the master. If not specified, GCP will default to a predetermined
	// computed value (currently n1-standard-4).
	// +kubebuilder:validation:Optional
	machineType?: null | string @go(MachineType,*string)

	// The name of a minimum generation of CPU family
	// for the master. If not specified, GCP will default to a predetermined computed value
	// for each zone. See the guide
	// for details about which CPU families are available (and defaulted) for each zone.
	// +kubebuilder:validation:Optional
	minCpuPlatform?: null | string @go(MinCPUPlatform,*string)

	// Specifies the number of master nodes to create.
	// If not specified, GCP will default to a predetermined computed value (currently 1).
	// +kubebuilder:validation:Optional
	numInstances?: null | float64 @go(NumInstances,*float64)
}

#MetastoreConfigInitParameters: {
	// Resource name of an existing Dataproc Metastore service.
	dataprocMetastoreService?: null | string @go(DataprocMetastoreService,*string)
}

#MetastoreConfigObservation: {
	// Resource name of an existing Dataproc Metastore service.
	dataprocMetastoreService?: null | string @go(DataprocMetastoreService,*string)
}

#MetastoreConfigParameters: {
	// Resource name of an existing Dataproc Metastore service.
	// +kubebuilder:validation:Optional
	dataprocMetastoreService?: null | string @go(DataprocMetastoreService,*string)
}

#MetricsInitParameters: {
	// One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
	metricOverrides?: [...null | string] @go(MetricOverrides,[]*string)

	// A source for the collection of Dataproc OSS metrics (see available OSS metrics).
	metricSource?: null | string @go(MetricSource,*string)
}

#MetricsObservation: {
	// One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
	metricOverrides?: [...null | string] @go(MetricOverrides,[]*string)

	// A source for the collection of Dataproc OSS metrics (see available OSS metrics).
	metricSource?: null | string @go(MetricSource,*string)
}

#MetricsParameters: {
	// One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
	// +kubebuilder:validation:Optional
	metricOverrides?: [...null | string] @go(MetricOverrides,[]*string)

	// A source for the collection of Dataproc OSS metrics (see available OSS metrics).
	// +kubebuilder:validation:Optional
	metricSource?: null | string @go(MetricSource,*string)
}

#NodeGroupAffinityInitParameters: {
	// The URI of a sole-tenant node group resource that the cluster will be created on.
	nodeGroupUri?: null | string @go(NodeGroupURI,*string)
}

#NodeGroupAffinityObservation: {
	// The URI of a sole-tenant node group resource that the cluster will be created on.
	nodeGroupUri?: null | string @go(NodeGroupURI,*string)
}

#NodeGroupAffinityParameters: {
	// The URI of a sole-tenant node group resource that the cluster will be created on.
	// +kubebuilder:validation:Optional
	nodeGroupUri?: null | string @go(NodeGroupURI,*string)
}

#NodePoolConfigInitParameters: {
	// The autoscaler configuration for this node pool.
	// The autoscaler is enabled only when a valid configuration is present.
	autoscaling?: [...#AutoscalingInitParameters] @go(Autoscaling,[]AutoscalingInitParameters)

	// The node pool configuration.
	config?: [...#ConfigInitParameters] @go(Config,[]ConfigInitParameters)

	// The list of Compute Engine zones where node pool nodes associated
	// with a Dataproc on GKE virtual cluster will be located.
	locations?: [...null | string] @go(Locations,[]*string)
}

#NodePoolConfigObservation: {
	// The autoscaler configuration for this node pool.
	// The autoscaler is enabled only when a valid configuration is present.
	autoscaling?: [...#AutoscalingObservation] @go(Autoscaling,[]AutoscalingObservation)

	// The node pool configuration.
	config?: [...#ConfigObservation] @go(Config,[]ConfigObservation)

	// The list of Compute Engine zones where node pool nodes associated
	// with a Dataproc on GKE virtual cluster will be located.
	locations?: [...null | string] @go(Locations,[]*string)
}

#NodePoolConfigParameters: {
	// The autoscaler configuration for this node pool.
	// The autoscaler is enabled only when a valid configuration is present.
	// +kubebuilder:validation:Optional
	autoscaling?: [...#AutoscalingParameters] @go(Autoscaling,[]AutoscalingParameters)

	// The node pool configuration.
	// +kubebuilder:validation:Optional
	config?: [...#ConfigParameters] @go(Config,[]ConfigParameters)

	// The list of Compute Engine zones where node pool nodes associated
	// with a Dataproc on GKE virtual cluster will be located.
	// +kubebuilder:validation:Optional
	locations: [...null | string] @go(Locations,[]*string)
}

#NodePoolTargetInitParameters: {
	// The target GKE node pool.
	nodePool?: null | string @go(NodePool,*string)

	// (Input only) The configuration for the GKE node pool.
	// If specified, Dataproc attempts to create a node pool with the specified shape.
	// If one with the same name already exists, it is verified against all specified fields.
	// If a field differs, the virtual cluster creation will fail.
	nodePoolConfig?: [...#NodePoolConfigInitParameters] @go(NodePoolConfig,[]NodePoolConfigInitParameters)

	// The roles associated with the GKE node pool.
	// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
	roles?: [...null | string] @go(Roles,[]*string)
}

#NodePoolTargetObservation: {
	// The target GKE node pool.
	nodePool?: null | string @go(NodePool,*string)

	// (Input only) The configuration for the GKE node pool.
	// If specified, Dataproc attempts to create a node pool with the specified shape.
	// If one with the same name already exists, it is verified against all specified fields.
	// If a field differs, the virtual cluster creation will fail.
	nodePoolConfig?: [...#NodePoolConfigObservation] @go(NodePoolConfig,[]NodePoolConfigObservation)

	// The roles associated with the GKE node pool.
	// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
	roles?: [...null | string] @go(Roles,[]*string)
}

#NodePoolTargetParameters: {
	// The target GKE node pool.
	// +kubebuilder:validation:Optional
	nodePool?: null | string @go(NodePool,*string)

	// (Input only) The configuration for the GKE node pool.
	// If specified, Dataproc attempts to create a node pool with the specified shape.
	// If one with the same name already exists, it is verified against all specified fields.
	// If a field differs, the virtual cluster creation will fail.
	// +kubebuilder:validation:Optional
	nodePoolConfig?: [...#NodePoolConfigParameters] @go(NodePoolConfig,[]NodePoolConfigParameters)

	// The roles associated with the GKE node pool.
	// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
	// +kubebuilder:validation:Optional
	roles: [...null | string] @go(Roles,[]*string)
}

#PreemptibleWorkerConfigDiskConfigInitParameters: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#PreemptibleWorkerConfigDiskConfigObservation: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#PreemptibleWorkerConfigDiskConfigParameters: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	// +kubebuilder:validation:Optional
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	// +kubebuilder:validation:Optional
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	// +kubebuilder:validation:Optional
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#PreemptibleWorkerConfigInitParameters: {
	// Disk Config
	diskConfig?: [...#PreemptibleWorkerConfigDiskConfigInitParameters] @go(DiskConfig,[]PreemptibleWorkerConfigDiskConfigInitParameters)

	// Specifies the number of preemptible nodes to create.
	// Defaults to 0.
	numInstances?: null | float64 @go(NumInstances,*float64)

	// Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE
	// Accepted values are:
	preemptibility?: null | string @go(Preemptibility,*string)
}

#PreemptibleWorkerConfigObservation: {
	// Disk Config
	diskConfig?: [...#PreemptibleWorkerConfigDiskConfigObservation] @go(DiskConfig,[]PreemptibleWorkerConfigDiskConfigObservation)

	// List of worker instance names which have been assigned
	// to the cluster.
	instanceNames?: [...null | string] @go(InstanceNames,[]*string)

	// Specifies the number of preemptible nodes to create.
	// Defaults to 0.
	numInstances?: null | float64 @go(NumInstances,*float64)

	// Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE
	// Accepted values are:
	preemptibility?: null | string @go(Preemptibility,*string)
}

#PreemptibleWorkerConfigParameters: {
	// Disk Config
	// +kubebuilder:validation:Optional
	diskConfig?: [...#PreemptibleWorkerConfigDiskConfigParameters] @go(DiskConfig,[]PreemptibleWorkerConfigDiskConfigParameters)

	// Specifies the number of preemptible nodes to create.
	// Defaults to 0.
	// +kubebuilder:validation:Optional
	numInstances?: null | float64 @go(NumInstances,*float64)

	// Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE
	// Accepted values are:
	// +kubebuilder:validation:Optional
	preemptibility?: null | string @go(Preemptibility,*string)
}

#ReservationAffinityInitParameters: {
	// Corresponds to the type of reservation consumption.
	consumeReservationType?: null | string @go(ConsumeReservationType,*string)

	// Corresponds to the label key of reservation resource.
	key?: null | string @go(Key,*string)

	// Corresponds to the label values of reservation resource.
	values?: [...null | string] @go(Values,[]*string)
}

#ReservationAffinityObservation: {
	// Corresponds to the type of reservation consumption.
	consumeReservationType?: null | string @go(ConsumeReservationType,*string)

	// Corresponds to the label key of reservation resource.
	key?: null | string @go(Key,*string)

	// Corresponds to the label values of reservation resource.
	values?: [...null | string] @go(Values,[]*string)
}

#ReservationAffinityParameters: {
	// Corresponds to the type of reservation consumption.
	// +kubebuilder:validation:Optional
	consumeReservationType?: null | string @go(ConsumeReservationType,*string)

	// Corresponds to the label key of reservation resource.
	// +kubebuilder:validation:Optional
	key?: null | string @go(Key,*string)

	// Corresponds to the label values of reservation resource.
	// +kubebuilder:validation:Optional
	values?: [...null | string] @go(Values,[]*string)
}

#SecurityConfigInitParameters: {
	// Kerberos Configuration
	kerberosConfig?: [...#KerberosConfigInitParameters] @go(KerberosConfig,[]KerberosConfigInitParameters)
}

#SecurityConfigObservation: {
	// Kerberos Configuration
	kerberosConfig?: [...#KerberosConfigObservation] @go(KerberosConfig,[]KerberosConfigObservation)
}

#SecurityConfigParameters: {
	// Kerberos Configuration
	// +kubebuilder:validation:Optional
	kerberosConfig: [...#KerberosConfigParameters] @go(KerberosConfig,[]KerberosConfigParameters)
}

#ShieldedInstanceConfigInitParameters: {
	// Defines whether instances have integrity monitoring enabled.
	enableIntegrityMonitoring?: null | bool @go(EnableIntegrityMonitoring,*bool)

	// Defines whether instances have Secure Boot enabled.
	enableSecureBoot?: null | bool @go(EnableSecureBoot,*bool)

	// Defines whether instances have the vTPM enabled.
	enableVtpm?: null | bool @go(EnableVtpm,*bool)
}

#ShieldedInstanceConfigObservation: {
	// Defines whether instances have integrity monitoring enabled.
	enableIntegrityMonitoring?: null | bool @go(EnableIntegrityMonitoring,*bool)

	// Defines whether instances have Secure Boot enabled.
	enableSecureBoot?: null | bool @go(EnableSecureBoot,*bool)

	// Defines whether instances have the vTPM enabled.
	enableVtpm?: null | bool @go(EnableVtpm,*bool)
}

#ShieldedInstanceConfigParameters: {
	// Defines whether instances have integrity monitoring enabled.
	// +kubebuilder:validation:Optional
	enableIntegrityMonitoring?: null | bool @go(EnableIntegrityMonitoring,*bool)

	// Defines whether instances have Secure Boot enabled.
	// +kubebuilder:validation:Optional
	enableSecureBoot?: null | bool @go(EnableSecureBoot,*bool)

	// Defines whether instances have the vTPM enabled.
	// +kubebuilder:validation:Optional
	enableVtpm?: null | bool @go(EnableVtpm,*bool)
}

#SoftwareConfigInitParameters: {
	// The Cloud Dataproc image version to use
	// for the cluster - this controls the sets of software versions
	// installed onto the nodes when you create clusters. If not specified, defaults to the
	// latest version. For a list of valid versions see
	// Cloud Dataproc versions
	imageVersion?: null | string @go(ImageVersion,*string)

	// The set of optional components to activate on the cluster. See Available Optional Components.
	optionalComponents?: [...null | string] @go(OptionalComponents,[]*string)

	// A list of override and additional properties (key/value pairs)
	// used to modify various aspects of the common configuration files used when creating
	// a cluster. For a list of valid properties please see
	// Cluster properties
	overrideProperties?: {[string]: null | string} @go(OverrideProperties,map[string]*string)
}

#SoftwareConfigObservation: {
	// The Cloud Dataproc image version to use
	// for the cluster - this controls the sets of software versions
	// installed onto the nodes when you create clusters. If not specified, defaults to the
	// latest version. For a list of valid versions see
	// Cloud Dataproc versions
	imageVersion?: null | string @go(ImageVersion,*string)

	// The set of optional components to activate on the cluster. See Available Optional Components.
	optionalComponents?: [...null | string] @go(OptionalComponents,[]*string)

	// A list of override and additional properties (key/value pairs)
	// used to modify various aspects of the common configuration files used when creating
	// a cluster. For a list of valid properties please see
	// Cluster properties
	overrideProperties?: {[string]: null | string} @go(OverrideProperties,map[string]*string)

	// The properties to set on daemon config files. Property keys are specified in prefix:property format,
	// for example spark:spark.kubernetes.container.image.
	properties?: {[string]: string} @go(Properties,map[string]string)
}

#SoftwareConfigParameters: {
	// The Cloud Dataproc image version to use
	// for the cluster - this controls the sets of software versions
	// installed onto the nodes when you create clusters. If not specified, defaults to the
	// latest version. For a list of valid versions see
	// Cloud Dataproc versions
	// +kubebuilder:validation:Optional
	imageVersion?: null | string @go(ImageVersion,*string)

	// The set of optional components to activate on the cluster. See Available Optional Components.
	// +kubebuilder:validation:Optional
	optionalComponents?: [...null | string] @go(OptionalComponents,[]*string)

	// A list of override and additional properties (key/value pairs)
	// used to modify various aspects of the common configuration files used when creating
	// a cluster. For a list of valid properties please see
	// Cluster properties
	// +kubebuilder:validation:Optional
	overrideProperties?: {[string]: null | string} @go(OverrideProperties,map[string]*string)
}

#SparkHistoryServerConfigInitParameters: {
	// Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
	dataprocCluster?: null | string @go(DataprocCluster,*string)
}

#SparkHistoryServerConfigObservation: {
	// Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
	dataprocCluster?: null | string @go(DataprocCluster,*string)
}

#SparkHistoryServerConfigParameters: {
	// Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
	// +kubebuilder:validation:Optional
	dataprocCluster?: null | string @go(DataprocCluster,*string)
}

#VirtualClusterConfigInitParameters: {
	// Configuration of auxiliary services used by this cluster.
	// Structure defined below.
	auxiliaryServicesConfig?: [...#AuxiliaryServicesConfigInitParameters] @go(AuxiliaryServicesConfig,[]AuxiliaryServicesConfigInitParameters)

	// The configuration for running the Dataproc cluster on Kubernetes.
	// Structure defined below.
	kubernetesClusterConfig?: [...#KubernetesClusterConfigInitParameters] @go(KubernetesClusterConfig,[]KubernetesClusterConfigInitParameters)

	// The Cloud Storage staging bucket used to stage files,
	// such as Hadoop jars, between client machines and the cluster.
	// Note: If you don't explicitly specify a staging_bucket
	// then GCP will auto create / assign one for you. However, you are not guaranteed
	// an auto generated bucket which is solely dedicated to your cluster; it may be shared
	// with other clusters in the same region/zone also choosing to use the auto generation
	// option.
	stagingBucket?: null | string @go(StagingBucket,*string)
}

#VirtualClusterConfigObservation: {
	// Configuration of auxiliary services used by this cluster.
	// Structure defined below.
	auxiliaryServicesConfig?: [...#AuxiliaryServicesConfigObservation] @go(AuxiliaryServicesConfig,[]AuxiliaryServicesConfigObservation)

	// The configuration for running the Dataproc cluster on Kubernetes.
	// Structure defined below.
	kubernetesClusterConfig?: [...#KubernetesClusterConfigObservation] @go(KubernetesClusterConfig,[]KubernetesClusterConfigObservation)

	// The Cloud Storage staging bucket used to stage files,
	// such as Hadoop jars, between client machines and the cluster.
	// Note: If you don't explicitly specify a staging_bucket
	// then GCP will auto create / assign one for you. However, you are not guaranteed
	// an auto generated bucket which is solely dedicated to your cluster; it may be shared
	// with other clusters in the same region/zone also choosing to use the auto generation
	// option.
	stagingBucket?: null | string @go(StagingBucket,*string)
}

#VirtualClusterConfigParameters: {
	// Configuration of auxiliary services used by this cluster.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	auxiliaryServicesConfig?: [...#AuxiliaryServicesConfigParameters] @go(AuxiliaryServicesConfig,[]AuxiliaryServicesConfigParameters)

	// The configuration for running the Dataproc cluster on Kubernetes.
	// Structure defined below.
	// +kubebuilder:validation:Optional
	kubernetesClusterConfig?: [...#KubernetesClusterConfigParameters] @go(KubernetesClusterConfig,[]KubernetesClusterConfigParameters)

	// The Cloud Storage staging bucket used to stage files,
	// such as Hadoop jars, between client machines and the cluster.
	// Note: If you don't explicitly specify a staging_bucket
	// then GCP will auto create / assign one for you. However, you are not guaranteed
	// an auto generated bucket which is solely dedicated to your cluster; it may be shared
	// with other clusters in the same region/zone also choosing to use the auto generation
	// option.
	// +kubebuilder:validation:Optional
	stagingBucket?: null | string @go(StagingBucket,*string)
}

#WorkerConfigAcceleratorsInitParameters: {
	// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
	acceleratorCount?: null | float64 @go(AcceleratorCount,*float64)

	// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
	acceleratorType?: null | string @go(AcceleratorType,*string)
}

#WorkerConfigAcceleratorsObservation: {
	// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
	acceleratorCount?: null | float64 @go(AcceleratorCount,*float64)

	// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
	acceleratorType?: null | string @go(AcceleratorType,*string)
}

#WorkerConfigAcceleratorsParameters: {
	// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
	// +kubebuilder:validation:Optional
	acceleratorCount?: null | float64 @go(AcceleratorCount,*float64)

	// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
	// +kubebuilder:validation:Optional
	acceleratorType?: null | string @go(AcceleratorType,*string)
}

#WorkerConfigDiskConfigInitParameters: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#WorkerConfigDiskConfigObservation: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

#WorkerConfigDiskConfigParameters: {
	// Size of the primary disk attached to each node, specified
	// in GB. The primary disk contains the boot volume and system libraries, and the
	// smallest allowed disk size is 10GB. GCP will default to a predetermined
	// computed value if not set (currently 500GB). Note: If SSDs are not
	// attached, it also contains the HDFS data blocks and Hadoop working directories.
	// +kubebuilder:validation:Optional
	bootDiskSizeGb?: null | float64 @go(BootDiskSizeGb,*float64)

	// The disk type of the primary disk attached to each node.
	// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
	// +kubebuilder:validation:Optional
	bootDiskType?: null | string @go(BootDiskType,*string)

	// The amount of local SSD disks that will be
	// attached to each master cluster node. Defaults to 0.
	// +kubebuilder:validation:Optional
	numLocalSsds?: null | float64 @go(NumLocalSsds,*float64)
}

// ClusterSpec defines the desired state of Cluster
#ClusterSpec: {
	forProvider: #ClusterParameters @go(ForProvider)

	// THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored
	// unless the relevant Crossplane feature flag is enabled, and may be
	// changed or removed without notice.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	initProvider?: #ClusterInitParameters @go(InitProvider)
}

// ClusterStatus defines the observed state of Cluster.
#ClusterStatus: {
	atProvider?: #ClusterObservation @go(AtProvider)
}

// Cluster is the Schema for the Clusters API. Manages a Cloud Dataproc cluster resource.
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:subresource:status
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,gcp}
#Cluster: {
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.name) || (has(self.initProvider) && has(self.initProvider.name))",message="spec.forProvider.name is a required parameter"
	spec:    #ClusterSpec   @go(Spec)
	status?: #ClusterStatus @go(Status)
}

// ClusterList contains a list of Clusters
#ClusterList: {
	items: [...#Cluster] @go(Items,[]Cluster)
}
