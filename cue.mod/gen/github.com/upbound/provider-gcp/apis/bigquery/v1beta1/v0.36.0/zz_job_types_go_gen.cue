// Code generated by cue get go. DO NOT EDIT.

//cue:generate cue get go github.com/upbound/provider-gcp/apis/bigquery/v1beta1

package v1beta1

#CopyInitParameters: {
	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	destinationEncryptionConfiguration?: [...#DestinationEncryptionConfigurationInitParameters] @go(DestinationEncryptionConfiguration,[]DestinationEncryptionConfigurationInitParameters)

	// The destination table.
	// Structure is documented below.
	destinationTable?: [...#DestinationTableInitParameters] @go(DestinationTable,[]DestinationTableInitParameters)

	// Source tables to copy.
	// Structure is documented below.
	sourceTables?: [...#SourceTablesInitParameters] @go(SourceTables,[]SourceTablesInitParameters)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#CopyObservation: {
	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	destinationEncryptionConfiguration?: [...#DestinationEncryptionConfigurationObservation] @go(DestinationEncryptionConfiguration,[]DestinationEncryptionConfigurationObservation)

	// The destination table.
	// Structure is documented below.
	destinationTable?: [...#DestinationTableObservation] @go(DestinationTable,[]DestinationTableObservation)

	// Source tables to copy.
	// Structure is documented below.
	sourceTables?: [...#SourceTablesObservation] @go(SourceTables,[]SourceTablesObservation)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#CopyParameters: {
	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	// +kubebuilder:validation:Optional
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	destinationEncryptionConfiguration?: [...#DestinationEncryptionConfigurationParameters] @go(DestinationEncryptionConfiguration,[]DestinationEncryptionConfigurationParameters)

	// The destination table.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	destinationTable?: [...#DestinationTableParameters] @go(DestinationTable,[]DestinationTableParameters)

	// Source tables to copy.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	sourceTables: [...#SourceTablesParameters] @go(SourceTables,[]SourceTablesParameters)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	// +kubebuilder:validation:Optional
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#DefaultDatasetInitParameters: {
	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#DefaultDatasetObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#DefaultDatasetParameters: {
	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)
}

#DestinationEncryptionConfigurationInitParameters: {
}

#DestinationEncryptionConfigurationObservation: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	kmsKeyName?: null | string @go(KMSKeyName,*string)

	// (Output)
	// Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
	kmsKeyVersion?: null | string @go(KMSKeyVersion,*string)
}

#DestinationEncryptionConfigurationParameters: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/kms/v1beta1.CryptoKey
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#DestinationTableInitParameters: {
	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#DestinationTableObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	tableId?: null | string @go(TableID,*string)
}

#DestinationTableParameters: {
	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	tableId?: null | string @go(TableID,*string)
}

#ErrorResultInitParameters: {
}

#ErrorResultObservation: {
	// The geographic location of the job. The default value is US.
	location?: null | string @go(Location,*string)

	// A human-readable description of the error.
	message?: null | string @go(Message,*string)

	// A short error code that summarizes the error.
	reason?: null | string @go(Reason,*string)
}

#ErrorResultParameters: {
}

#ErrorsInitParameters: {
}

#ErrorsObservation: {
	// The geographic location of the job. The default value is US.
	location?: null | string @go(Location,*string)

	// A human-readable description of the error.
	message?: null | string @go(Message,*string)

	// A short error code that summarizes the error.
	reason?: null | string @go(Reason,*string)
}

#ErrorsParameters: {
}

#ExtractInitParameters: {
	// The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
	// The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
	compression?: null | string @go(Compression,*string)

	// The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
	// The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
	// The default value for models is SAVED_MODEL.
	destinationFormat?: null | string @go(DestinationFormat,*string)

	// A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
	destinationUris?: [...null | string] @go(DestinationUris,[]*string)

	// When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
	// Default is ','
	fieldDelimiter?: null | string @go(FieldDelimiter,*string)

	// Whether to print out a header row in the results. Default is true.
	printHeader?: null | bool @go(PrintHeader,*bool)

	// A reference to the model being exported.
	// Structure is documented below.
	sourceModel?: [...#SourceModelInitParameters] @go(SourceModel,[]SourceModelInitParameters)

	// A reference to the table being exported.
	// Structure is documented below.
	sourceTable?: [...#SourceTableInitParameters] @go(SourceTable,[]SourceTableInitParameters)

	// Whether to use logical types when extracting to AVRO format.
	useAvroLogicalTypes?: null | bool @go(UseAvroLogicalTypes,*bool)
}

#ExtractObservation: {
	// The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
	// The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
	compression?: null | string @go(Compression,*string)

	// The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
	// The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
	// The default value for models is SAVED_MODEL.
	destinationFormat?: null | string @go(DestinationFormat,*string)

	// A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
	destinationUris?: [...null | string] @go(DestinationUris,[]*string)

	// When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
	// Default is ','
	fieldDelimiter?: null | string @go(FieldDelimiter,*string)

	// Whether to print out a header row in the results. Default is true.
	printHeader?: null | bool @go(PrintHeader,*bool)

	// A reference to the model being exported.
	// Structure is documented below.
	sourceModel?: [...#SourceModelObservation] @go(SourceModel,[]SourceModelObservation)

	// A reference to the table being exported.
	// Structure is documented below.
	sourceTable?: [...#SourceTableObservation] @go(SourceTable,[]SourceTableObservation)

	// Whether to use logical types when extracting to AVRO format.
	useAvroLogicalTypes?: null | bool @go(UseAvroLogicalTypes,*bool)
}

#ExtractParameters: {
	// The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
	// The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
	// +kubebuilder:validation:Optional
	compression?: null | string @go(Compression,*string)

	// The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
	// The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
	// The default value for models is SAVED_MODEL.
	// +kubebuilder:validation:Optional
	destinationFormat?: null | string @go(DestinationFormat,*string)

	// A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
	// +kubebuilder:validation:Optional
	destinationUris: [...null | string] @go(DestinationUris,[]*string)

	// When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
	// Default is ','
	// +kubebuilder:validation:Optional
	fieldDelimiter?: null | string @go(FieldDelimiter,*string)

	// Whether to print out a header row in the results. Default is true.
	// +kubebuilder:validation:Optional
	printHeader?: null | bool @go(PrintHeader,*bool)

	// A reference to the model being exported.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	sourceModel?: [...#SourceModelParameters] @go(SourceModel,[]SourceModelParameters)

	// A reference to the table being exported.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	sourceTable?: [...#SourceTableParameters] @go(SourceTable,[]SourceTableParameters)

	// Whether to use logical types when extracting to AVRO format.
	// +kubebuilder:validation:Optional
	useAvroLogicalTypes?: null | bool @go(UseAvroLogicalTypes,*bool)
}

#JobInitParameters: {
	// Copies a table.
	// Structure is documented below.
	copy?: [...#CopyInitParameters] @go(Copy,[]CopyInitParameters)

	// Configures an extract job.
	// Structure is documented below.
	extract?: [...#ExtractInitParameters] @go(Extract,[]ExtractInitParameters)

	// The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.
	jobId?: null | string @go(JobID,*string)

	// Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.
	jobTimeoutMs?: null | string @go(JobTimeoutMs,*string)

	// The labels associated with this job. You can use these to organize and group your jobs.
	labels?: {[string]: null | string} @go(Labels,map[string]*string)

	// Configures a load job.
	// Structure is documented below.
	load?: [...#LoadInitParameters] @go(Load,[]LoadInitParameters)

	// The geographic location of the job. The default value is US.
	location?: null | string @go(Location,*string)

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	project?: null | string @go(Project,*string)

	// Configures a query job.
	// Structure is documented below.
	query?: [...#QueryInitParameters] @go(Query,[]QueryInitParameters)
}

#JobObservation: {
	// Copies a table.
	// Structure is documented below.
	copy?: [...#CopyObservation] @go(Copy,[]CopyObservation)

	// Configures an extract job.
	// Structure is documented below.
	extract?: [...#ExtractObservation] @go(Extract,[]ExtractObservation)

	// an identifier for the resource with format projects/{{project}}/jobs/{{job_id}}
	id?: null | string @go(ID,*string)

	// The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.
	jobId?: null | string @go(JobID,*string)

	// Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.
	jobTimeoutMs?: null | string @go(JobTimeoutMs,*string)

	// (Output)
	// The type of the job.
	jobType?: null | string @go(JobType,*string)

	// The labels associated with this job. You can use these to organize and group your jobs.
	labels?: {[string]: null | string} @go(Labels,map[string]*string)

	// Configures a load job.
	// Structure is documented below.
	load?: [...#LoadObservation] @go(Load,[]LoadObservation)

	// The geographic location of the job. The default value is US.
	location?: null | string @go(Location,*string)

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	project?: null | string @go(Project,*string)

	// Configures a query job.
	// Structure is documented below.
	query?: [...#QueryObservation] @go(Query,[]QueryObservation)

	// The status of this job. Examine this value when polling an asynchronous job to see if the job is complete.
	// Structure is documented below.
	status?: [...#StatusObservation] @go(Status,[]StatusObservation)

	// Email address of the user who ran the job.
	userEmail?: null | string @go(UserEmail,*string)
}

#JobParameters: {
	// Copies a table.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	copy?: [...#CopyParameters] @go(Copy,[]CopyParameters)

	// Configures an extract job.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	extract?: [...#ExtractParameters] @go(Extract,[]ExtractParameters)

	// The ID of the job. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters.
	// +kubebuilder:validation:Optional
	jobId?: null | string @go(JobID,*string)

	// Job timeout in milliseconds. If this time limit is exceeded, BigQuery may attempt to terminate the job.
	// +kubebuilder:validation:Optional
	jobTimeoutMs?: null | string @go(JobTimeoutMs,*string)

	// The labels associated with this job. You can use these to organize and group your jobs.
	// +kubebuilder:validation:Optional
	labels?: {[string]: null | string} @go(Labels,map[string]*string)

	// Configures a load job.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	load?: [...#LoadParameters] @go(Load,[]LoadParameters)

	// The geographic location of the job. The default value is US.
	// +kubebuilder:validation:Optional
	location?: null | string @go(Location,*string)

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	// +kubebuilder:validation:Optional
	project?: null | string @go(Project,*string)

	// Configures a query job.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	query?: [...#QueryParameters] @go(Query,[]QueryParameters)
}

#LoadDestinationEncryptionConfigurationInitParameters: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#LoadDestinationEncryptionConfigurationObservation: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	kmsKeyName?: null | string @go(KMSKeyName,*string)

	// (Output)
	// Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
	kmsKeyVersion?: null | string @go(KMSKeyVersion,*string)
}

#LoadDestinationEncryptionConfigurationParameters: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +kubebuilder:validation:Optional
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#LoadDestinationTableInitParameters: {
	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#LoadDestinationTableObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	tableId?: null | string @go(TableID,*string)
}

#LoadDestinationTableParameters: {
	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	tableId?: null | string @go(TableID,*string)
}

#LoadInitParameters: {
	// Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
	// If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
	// an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
	allowJaggedRows?: null | bool @go(AllowJaggedRows,*bool)

	// Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
	// The default value is false.
	allowQuotedNewlines?: null | bool @go(AllowQuotedNewlines,*bool)

	// Indicates if we should automatically infer the options and schema for CSV and JSON sources.
	autodetect?: null | bool @go(Autodetect,*bool)

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	destinationEncryptionConfiguration?: [...#LoadDestinationEncryptionConfigurationInitParameters] @go(DestinationEncryptionConfiguration,[]LoadDestinationEncryptionConfigurationInitParameters)

	// The destination table to load the data into.
	// Structure is documented below.
	destinationTable?: [...#LoadDestinationTableInitParameters] @go(DestinationTable,[]LoadDestinationTableInitParameters)

	// The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
	// The default value is UTF-8. BigQuery decodes the data after the raw, binary data
	// has been split using the values of the quote and fieldDelimiter properties.
	encoding?: null | string @go(Encoding,*string)

	// The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
	// To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
	// the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
	// data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
	// The default value is a comma (',').
	fieldDelimiter?: null | string @go(FieldDelimiter,*string)

	// Indicates if BigQuery should allow extra values that are not represented in the table schema.
	// If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
	// and if there are too many bad records, an invalid error is returned in the job result.
	// The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
	// CSV: Trailing columns
	// JSON: Named values that don't match any column names
	ignoreUnknownValues?: null | bool @go(IgnoreUnknownValues,*bool)

	// If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
	// For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
	// GeoJSON: set to GEOJSON.
	jsonExtension?: null | string @go(JSONExtension,*string)

	// The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
	// an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
	maxBadRecords?: null | float64 @go(MaxBadRecords,*float64)

	// Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
	// when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
	// empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
	// an empty value.
	nullMarker?: null | string @go(NullMarker,*string)

	// Parquet Options for load and make external tables.
	// Structure is documented below.
	parquetOptions?: [...#ParquetOptionsInitParameters] @go(ParquetOptions,[]ParquetOptionsInitParameters)

	// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
	// Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
	// If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
	projectionFields?: [...null | string] @go(ProjectionFields,[]*string)

	// The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
	// and then uses the first byte of the encoded string to split the data in its raw, binary state.
	// The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
	// If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
	quote?: null | string @go(Quote,*string)

	// Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
	// supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
	// For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	schemaUpdateOptions?: [...null | string] @go(SchemaUpdateOptions,[]*string)

	// The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
	// The default value is 0. This property is useful if you have header rows in the file that should be skipped.
	// When autodetect is on, the behavior is the following:
	// skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
	// the row is read as data. Otherwise data is read starting from the second row.
	// skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
	// skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
	// row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
	skipLeadingRows?: null | float64 @go(SkipLeadingRows,*float64)

	// The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
	// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
	// For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
	// The default value is CSV.
	sourceFormat?: null | string @go(SourceFormat,*string)

	// The fully-qualified URIs that point to your data in Google Cloud.
	// For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character
	// and it must come after the 'bucket' name. Size limits related to load jobs apply
	// to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
	// specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
	// For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.
	sourceUris?: [...null | string] @go(SourceUris,[]*string)

	// Time-based partitioning specification for the destination table.
	// Structure is documented below.
	timePartitioning?: [...#TimePartitioningInitParameters] @go(TimePartitioning,[]TimePartitioningInitParameters)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#LoadObservation: {
	// Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
	// If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
	// an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
	allowJaggedRows?: null | bool @go(AllowJaggedRows,*bool)

	// Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
	// The default value is false.
	allowQuotedNewlines?: null | bool @go(AllowQuotedNewlines,*bool)

	// Indicates if we should automatically infer the options and schema for CSV and JSON sources.
	autodetect?: null | bool @go(Autodetect,*bool)

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	destinationEncryptionConfiguration?: [...#LoadDestinationEncryptionConfigurationObservation] @go(DestinationEncryptionConfiguration,[]LoadDestinationEncryptionConfigurationObservation)

	// The destination table to load the data into.
	// Structure is documented below.
	destinationTable?: [...#LoadDestinationTableObservation] @go(DestinationTable,[]LoadDestinationTableObservation)

	// The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
	// The default value is UTF-8. BigQuery decodes the data after the raw, binary data
	// has been split using the values of the quote and fieldDelimiter properties.
	encoding?: null | string @go(Encoding,*string)

	// The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
	// To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
	// the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
	// data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
	// The default value is a comma (',').
	fieldDelimiter?: null | string @go(FieldDelimiter,*string)

	// Indicates if BigQuery should allow extra values that are not represented in the table schema.
	// If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
	// and if there are too many bad records, an invalid error is returned in the job result.
	// The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
	// CSV: Trailing columns
	// JSON: Named values that don't match any column names
	ignoreUnknownValues?: null | bool @go(IgnoreUnknownValues,*bool)

	// If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
	// For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
	// GeoJSON: set to GEOJSON.
	jsonExtension?: null | string @go(JSONExtension,*string)

	// The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
	// an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
	maxBadRecords?: null | float64 @go(MaxBadRecords,*float64)

	// Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
	// when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
	// empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
	// an empty value.
	nullMarker?: null | string @go(NullMarker,*string)

	// Parquet Options for load and make external tables.
	// Structure is documented below.
	parquetOptions?: [...#ParquetOptionsObservation] @go(ParquetOptions,[]ParquetOptionsObservation)

	// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
	// Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
	// If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
	projectionFields?: [...null | string] @go(ProjectionFields,[]*string)

	// The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
	// and then uses the first byte of the encoded string to split the data in its raw, binary state.
	// The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
	// If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
	quote?: null | string @go(Quote,*string)

	// Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
	// supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
	// For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	schemaUpdateOptions?: [...null | string] @go(SchemaUpdateOptions,[]*string)

	// The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
	// The default value is 0. This property is useful if you have header rows in the file that should be skipped.
	// When autodetect is on, the behavior is the following:
	// skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
	// the row is read as data. Otherwise data is read starting from the second row.
	// skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
	// skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
	// row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
	skipLeadingRows?: null | float64 @go(SkipLeadingRows,*float64)

	// The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
	// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
	// For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
	// The default value is CSV.
	sourceFormat?: null | string @go(SourceFormat,*string)

	// The fully-qualified URIs that point to your data in Google Cloud.
	// For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character
	// and it must come after the 'bucket' name. Size limits related to load jobs apply
	// to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
	// specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
	// For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.
	sourceUris?: [...null | string] @go(SourceUris,[]*string)

	// Time-based partitioning specification for the destination table.
	// Structure is documented below.
	timePartitioning?: [...#TimePartitioningObservation] @go(TimePartitioning,[]TimePartitioningObservation)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#LoadParameters: {
	// Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
	// If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
	// an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
	// +kubebuilder:validation:Optional
	allowJaggedRows?: null | bool @go(AllowJaggedRows,*bool)

	// Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
	// The default value is false.
	// +kubebuilder:validation:Optional
	allowQuotedNewlines?: null | bool @go(AllowQuotedNewlines,*bool)

	// Indicates if we should automatically infer the options and schema for CSV and JSON sources.
	// +kubebuilder:validation:Optional
	autodetect?: null | bool @go(Autodetect,*bool)

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	// +kubebuilder:validation:Optional
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	destinationEncryptionConfiguration?: [...#LoadDestinationEncryptionConfigurationParameters] @go(DestinationEncryptionConfiguration,[]LoadDestinationEncryptionConfigurationParameters)

	// The destination table to load the data into.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	destinationTable: [...#LoadDestinationTableParameters] @go(DestinationTable,[]LoadDestinationTableParameters)

	// The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
	// The default value is UTF-8. BigQuery decodes the data after the raw, binary data
	// has been split using the values of the quote and fieldDelimiter properties.
	// +kubebuilder:validation:Optional
	encoding?: null | string @go(Encoding,*string)

	// The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
	// To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
	// the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
	// data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
	// The default value is a comma (',').
	// +kubebuilder:validation:Optional
	fieldDelimiter?: null | string @go(FieldDelimiter,*string)

	// Indicates if BigQuery should allow extra values that are not represented in the table schema.
	// If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
	// and if there are too many bad records, an invalid error is returned in the job result.
	// The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
	// CSV: Trailing columns
	// JSON: Named values that don't match any column names
	// +kubebuilder:validation:Optional
	ignoreUnknownValues?: null | bool @go(IgnoreUnknownValues,*bool)

	// If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
	// For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
	// GeoJSON: set to GEOJSON.
	// +kubebuilder:validation:Optional
	jsonExtension?: null | string @go(JSONExtension,*string)

	// The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
	// an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
	// +kubebuilder:validation:Optional
	maxBadRecords?: null | float64 @go(MaxBadRecords,*float64)

	// Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
	// when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
	// empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
	// an empty value.
	// +kubebuilder:validation:Optional
	nullMarker?: null | string @go(NullMarker,*string)

	// Parquet Options for load and make external tables.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	parquetOptions?: [...#ParquetOptionsParameters] @go(ParquetOptions,[]ParquetOptionsParameters)

	// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
	// Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
	// If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
	// +kubebuilder:validation:Optional
	projectionFields?: [...null | string] @go(ProjectionFields,[]*string)

	// The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
	// and then uses the first byte of the encoded string to split the data in its raw, binary state.
	// The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
	// If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
	// +kubebuilder:validation:Optional
	quote?: null | string @go(Quote,*string)

	// Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
	// supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
	// For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	// +kubebuilder:validation:Optional
	schemaUpdateOptions?: [...null | string] @go(SchemaUpdateOptions,[]*string)

	// The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
	// The default value is 0. This property is useful if you have header rows in the file that should be skipped.
	// When autodetect is on, the behavior is the following:
	// skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
	// the row is read as data. Otherwise data is read starting from the second row.
	// skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
	// skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
	// row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
	// +kubebuilder:validation:Optional
	skipLeadingRows?: null | float64 @go(SkipLeadingRows,*float64)

	// The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
	// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
	// For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
	// The default value is CSV.
	// +kubebuilder:validation:Optional
	sourceFormat?: null | string @go(SourceFormat,*string)

	// The fully-qualified URIs that point to your data in Google Cloud.
	// For Google Cloud Storage URIs: Each URI can contain one '*' wildcard character
	// and it must come after the 'bucket' name. Size limits related to load jobs apply
	// to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
	// specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
	// For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '*' wildcard character is not allowed.
	// +kubebuilder:validation:Optional
	sourceUris: [...null | string] @go(SourceUris,[]*string)

	// Time-based partitioning specification for the destination table.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	timePartitioning?: [...#TimePartitioningParameters] @go(TimePartitioning,[]TimePartitioningParameters)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	// +kubebuilder:validation:Optional
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#ParquetOptionsInitParameters: {
	// If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
	enableListInference?: null | bool @go(EnableListInference,*bool)

	// If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
	enumAsString?: null | bool @go(EnumAsString,*bool)
}

#ParquetOptionsObservation: {
	// If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
	enableListInference?: null | bool @go(EnableListInference,*bool)

	// If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
	enumAsString?: null | bool @go(EnumAsString,*bool)
}

#ParquetOptionsParameters: {
	// If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
	// +kubebuilder:validation:Optional
	enableListInference?: null | bool @go(EnableListInference,*bool)

	// If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
	// +kubebuilder:validation:Optional
	enumAsString?: null | bool @go(EnumAsString,*bool)
}

#QueryDestinationEncryptionConfigurationInitParameters: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#QueryDestinationEncryptionConfigurationObservation: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	kmsKeyName?: null | string @go(KMSKeyName,*string)

	// (Output)
	// Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
	kmsKeyVersion?: null | string @go(KMSKeyVersion,*string)
}

#QueryDestinationEncryptionConfigurationParameters: {
	// Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
	// The BigQuery Service Account associated with your project requires access to this encryption key.
	// +kubebuilder:validation:Optional
	kmsKeyName?: null | string @go(KMSKeyName,*string)
}

#QueryDestinationTableInitParameters: {
	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#QueryDestinationTableObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	tableId?: null | string @go(TableID,*string)
}

#QueryDestinationTableParameters: {
	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	tableId?: null | string @go(TableID,*string)
}

#QueryInitParameters: {
	// If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
	// Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
	// However, you must still set destinationTable when result size exceeds the allowed maximum response size.
	allowLargeResults?: null | bool @go(AllowLargeResults,*bool)

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
	// Structure is documented below.
	defaultDataset?: [...#DefaultDatasetInitParameters] @go(DefaultDataset,[]DefaultDatasetInitParameters)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	destinationEncryptionConfiguration?: [...#QueryDestinationEncryptionConfigurationInitParameters] @go(DestinationEncryptionConfiguration,[]QueryDestinationEncryptionConfigurationInitParameters)

	// Describes the table where the query results should be stored.
	// This property must be set for large results that exceed the maximum response size.
	// For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
	// Structure is documented below.
	destinationTable?: [...#QueryDestinationTableInitParameters] @go(DestinationTable,[]QueryDestinationTableInitParameters)

	// If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
	// allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
	flattenResults?: null | bool @go(FlattenResults,*bool)

	// Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	maximumBillingTier?: null | float64 @go(MaximumBillingTier,*float64)

	// Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	maximumBytesBilled?: null | string @go(MaximumBytesBilled,*string)

	// Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
	parameterMode?: null | string @go(ParameterMode,*string)

	// Specifies a priority for the query.
	// Default value is INTERACTIVE.
	// Possible values are: INTERACTIVE, BATCH.
	priority?: null | string @go(Priority,*string)

	// SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
	// NOTE: queries containing DML language
	// (DELETE, UPDATE, MERGE, INSERT) must specify create_disposition = "" and write_disposition = "".
	query?: null | string @go(Query,*string)

	// Allows the schema of the destination table to be updated as a side effect of the query job.
	// Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
	// specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
	// One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	schemaUpdateOptions?: [...null | string] @go(SchemaUpdateOptions,[]*string)

	// Options controlling the execution of scripts.
	// Structure is documented below.
	scriptOptions?: [...#ScriptOptionsInitParameters] @go(ScriptOptions,[]ScriptOptionsInitParameters)

	// Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
	// If set to false, the query will use BigQuery's standard SQL.
	useLegacySql?: null | bool @go(UseLegacySQL,*bool)

	// Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
	// tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
	// The default value is true.
	useQueryCache?: null | bool @go(UseQueryCache,*bool)

	// Describes user-defined function resources used in the query.
	// Structure is documented below.
	userDefinedFunctionResources?: [...#UserDefinedFunctionResourcesInitParameters] @go(UserDefinedFunctionResources,[]UserDefinedFunctionResourcesInitParameters)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#QueryObservation: {
	// If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
	// Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
	// However, you must still set destinationTable when result size exceeds the allowed maximum response size.
	allowLargeResults?: null | bool @go(AllowLargeResults,*bool)

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
	// Structure is documented below.
	defaultDataset?: [...#DefaultDatasetObservation] @go(DefaultDataset,[]DefaultDatasetObservation)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	destinationEncryptionConfiguration?: [...#QueryDestinationEncryptionConfigurationObservation] @go(DestinationEncryptionConfiguration,[]QueryDestinationEncryptionConfigurationObservation)

	// Describes the table where the query results should be stored.
	// This property must be set for large results that exceed the maximum response size.
	// For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
	// Structure is documented below.
	destinationTable?: [...#QueryDestinationTableObservation] @go(DestinationTable,[]QueryDestinationTableObservation)

	// If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
	// allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
	flattenResults?: null | bool @go(FlattenResults,*bool)

	// Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	maximumBillingTier?: null | float64 @go(MaximumBillingTier,*float64)

	// Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	maximumBytesBilled?: null | string @go(MaximumBytesBilled,*string)

	// Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
	parameterMode?: null | string @go(ParameterMode,*string)

	// Specifies a priority for the query.
	// Default value is INTERACTIVE.
	// Possible values are: INTERACTIVE, BATCH.
	priority?: null | string @go(Priority,*string)

	// SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
	// NOTE: queries containing DML language
	// (DELETE, UPDATE, MERGE, INSERT) must specify create_disposition = "" and write_disposition = "".
	query?: null | string @go(Query,*string)

	// Allows the schema of the destination table to be updated as a side effect of the query job.
	// Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
	// specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
	// One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	schemaUpdateOptions?: [...null | string] @go(SchemaUpdateOptions,[]*string)

	// Options controlling the execution of scripts.
	// Structure is documented below.
	scriptOptions?: [...#ScriptOptionsObservation] @go(ScriptOptions,[]ScriptOptionsObservation)

	// Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
	// If set to false, the query will use BigQuery's standard SQL.
	useLegacySql?: null | bool @go(UseLegacySQL,*bool)

	// Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
	// tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
	// The default value is true.
	useQueryCache?: null | bool @go(UseQueryCache,*bool)

	// Describes user-defined function resources used in the query.
	// Structure is documented below.
	userDefinedFunctionResources?: [...#UserDefinedFunctionResourcesObservation] @go(UserDefinedFunctionResources,[]UserDefinedFunctionResourcesObservation)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#QueryParameters: {
	// If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
	// Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
	// However, you must still set destinationTable when result size exceeds the allowed maximum response size.
	// +kubebuilder:validation:Optional
	allowLargeResults?: null | bool @go(AllowLargeResults,*bool)

	// Specifies whether the job is allowed to create new tables. The following values are supported:
	// CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
	// CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
	// Creation, truncation and append actions occur as one atomic update upon job completion
	// Default value is CREATE_IF_NEEDED.
	// Possible values are: CREATE_IF_NEEDED, CREATE_NEVER.
	// +kubebuilder:validation:Optional
	createDisposition?: null | string @go(CreateDisposition,*string)

	// Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	defaultDataset?: [...#DefaultDatasetParameters] @go(DefaultDataset,[]DefaultDatasetParameters)

	// Custom encryption configuration (e.g., Cloud KMS keys)
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	destinationEncryptionConfiguration?: [...#QueryDestinationEncryptionConfigurationParameters] @go(DestinationEncryptionConfiguration,[]QueryDestinationEncryptionConfigurationParameters)

	// Describes the table where the query results should be stored.
	// This property must be set for large results that exceed the maximum response size.
	// For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	destinationTable?: [...#QueryDestinationTableParameters] @go(DestinationTable,[]QueryDestinationTableParameters)

	// If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
	// allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
	// +kubebuilder:validation:Optional
	flattenResults?: null | bool @go(FlattenResults,*bool)

	// Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	// +kubebuilder:validation:Optional
	maximumBillingTier?: null | float64 @go(MaximumBillingTier,*float64)

	// Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
	// If unspecified, this will be set to your project default.
	// +kubebuilder:validation:Optional
	maximumBytesBilled?: null | string @go(MaximumBytesBilled,*string)

	// Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
	// +kubebuilder:validation:Optional
	parameterMode?: null | string @go(ParameterMode,*string)

	// Specifies a priority for the query.
	// Default value is INTERACTIVE.
	// Possible values are: INTERACTIVE, BATCH.
	// +kubebuilder:validation:Optional
	priority?: null | string @go(Priority,*string)

	// SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
	// NOTE: queries containing DML language
	// (DELETE, UPDATE, MERGE, INSERT) must specify create_disposition = "" and write_disposition = "".
	// +kubebuilder:validation:Optional
	query?: null | string @go(Query,*string)

	// Allows the schema of the destination table to be updated as a side effect of the query job.
	// Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
	// when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
	// specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
	// One or more of the following values are specified:
	// ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
	// ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
	// +kubebuilder:validation:Optional
	schemaUpdateOptions?: [...null | string] @go(SchemaUpdateOptions,[]*string)

	// Options controlling the execution of scripts.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	scriptOptions?: [...#ScriptOptionsParameters] @go(ScriptOptions,[]ScriptOptionsParameters)

	// Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
	// If set to false, the query will use BigQuery's standard SQL.
	// +kubebuilder:validation:Optional
	useLegacySql?: null | bool @go(UseLegacySQL,*bool)

	// Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
	// tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
	// The default value is true.
	// +kubebuilder:validation:Optional
	useQueryCache?: null | bool @go(UseQueryCache,*bool)

	// Describes user-defined function resources used in the query.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	userDefinedFunctionResources?: [...#UserDefinedFunctionResourcesParameters] @go(UserDefinedFunctionResources,[]UserDefinedFunctionResourcesParameters)

	// Specifies the action that occurs if the destination table already exists. The following values are supported:
	// WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
	// WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
	// WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
	// Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
	// Creation, truncation and append actions occur as one atomic update upon job completion.
	// Default value is WRITE_EMPTY.
	// Possible values are: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY.
	// +kubebuilder:validation:Optional
	writeDisposition?: null | string @go(WriteDisposition,*string)
}

#ScriptOptionsInitParameters: {
	// Determines which statement in the script represents the "key result",
	// used to populate the schema and query results of the script job.
	// Possible values are: LAST, FIRST_SELECT.
	keyResultStatement?: null | string @go(KeyResultStatement,*string)

	// Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
	statementByteBudget?: null | string @go(StatementByteBudget,*string)

	// Timeout period for each statement in a script.
	statementTimeoutMs?: null | string @go(StatementTimeoutMs,*string)
}

#ScriptOptionsObservation: {
	// Determines which statement in the script represents the "key result",
	// used to populate the schema and query results of the script job.
	// Possible values are: LAST, FIRST_SELECT.
	keyResultStatement?: null | string @go(KeyResultStatement,*string)

	// Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
	statementByteBudget?: null | string @go(StatementByteBudget,*string)

	// Timeout period for each statement in a script.
	statementTimeoutMs?: null | string @go(StatementTimeoutMs,*string)
}

#ScriptOptionsParameters: {
	// Determines which statement in the script represents the "key result",
	// used to populate the schema and query results of the script job.
	// Possible values are: LAST, FIRST_SELECT.
	// +kubebuilder:validation:Optional
	keyResultStatement?: null | string @go(KeyResultStatement,*string)

	// Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
	// +kubebuilder:validation:Optional
	statementByteBudget?: null | string @go(StatementByteBudget,*string)

	// Timeout period for each statement in a script.
	// +kubebuilder:validation:Optional
	statementTimeoutMs?: null | string @go(StatementTimeoutMs,*string)
}

#SourceModelInitParameters: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the model.
	modelId?: null | string @go(ModelID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#SourceModelObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the model.
	modelId?: null | string @go(ModelID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#SourceModelParameters: {
	// The ID of the dataset containing this table.
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the model.
	// +kubebuilder:validation:Optional
	modelId?: null | string @go(ModelID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)
}

#SourceTableInitParameters: {
	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)
}

#SourceTableObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	tableId?: null | string @go(TableID,*string)
}

#SourceTableParameters: {
	// The ID of the dataset containing this table.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Dataset
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +crossplane:generate:reference:type=github.com/upbound/provider-gcp/apis/bigquery/v1beta1.Table
	// +crossplane:generate:reference:extractor=github.com/upbound/upjet/pkg/resource.ExtractResourceID()
	// +kubebuilder:validation:Optional
	tableId?: null | string @go(TableID,*string)
}

#SourceTablesInitParameters: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	tableId?: null | string @go(TableID,*string)
}

#SourceTablesObservation: {
	// The ID of the dataset containing this table.
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	tableId?: null | string @go(TableID,*string)
}

#SourceTablesParameters: {
	// The ID of the dataset containing this table.
	// +kubebuilder:validation:Optional
	datasetId?: null | string @go(DatasetID,*string)

	// The ID of the project containing this table.
	// +kubebuilder:validation:Optional
	projectId?: null | string @go(ProjectID,*string)

	// The table. Can be specified {{table_id}} if project_id and dataset_id are also set,
	// or of the form projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}} if not.
	// +kubebuilder:validation:Optional
	tableId?: null | string @go(TableID,*string)
}

#StatusInitParameters: {
}

#StatusObservation: {
	// (Output)
	// Final error result of the job. If present, indicates that the job has completed and was unsuccessful.
	// Structure is documented below.
	errorResult?: [...#ErrorResultObservation] @go(ErrorResult,[]ErrorResultObservation)

	// (Output)
	// The first errors encountered during the running of the job. The final message
	// includes the number of errors that caused the process to stop. Errors here do
	// not necessarily mean that the job has not completed or was unsuccessful.
	// Structure is documented below.
	errors?: [...#ErrorsObservation] @go(Errors,[]ErrorsObservation)

	// (Output)
	// Running state of the job. Valid states include 'PENDING', 'RUNNING', and 'DONE'.
	state?: null | string @go(State,*string)
}

#StatusParameters: {
}

#TimePartitioningInitParameters: {
	// Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
	expirationMs?: null | string @go(ExpirationMs,*string)

	// If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
	// The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
	// A wrapper is used here because an empty string is an invalid value.
	field?: null | string @go(Field,*string)

	// The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
	// but in OnePlatform the field will be treated as unset.
	type?: null | string @go(Type,*string)
}

#TimePartitioningObservation: {
	// Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
	expirationMs?: null | string @go(ExpirationMs,*string)

	// If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
	// The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
	// A wrapper is used here because an empty string is an invalid value.
	field?: null | string @go(Field,*string)

	// The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
	// but in OnePlatform the field will be treated as unset.
	type?: null | string @go(Type,*string)
}

#TimePartitioningParameters: {
	// Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
	// +kubebuilder:validation:Optional
	expirationMs?: null | string @go(ExpirationMs,*string)

	// If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
	// The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
	// A wrapper is used here because an empty string is an invalid value.
	// +kubebuilder:validation:Optional
	field?: null | string @go(Field,*string)

	// The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
	// but in OnePlatform the field will be treated as unset.
	// +kubebuilder:validation:Optional
	type?: null | string @go(Type,*string)
}

#UserDefinedFunctionResourcesInitParameters: {
	// An inline resource that contains code for a user-defined function (UDF).
	// Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
	inlineCode?: null | string @go(InlineCode,*string)

	// A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
	resourceUri?: null | string @go(ResourceURI,*string)
}

#UserDefinedFunctionResourcesObservation: {
	// An inline resource that contains code for a user-defined function (UDF).
	// Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
	inlineCode?: null | string @go(InlineCode,*string)

	// A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
	resourceUri?: null | string @go(ResourceURI,*string)
}

#UserDefinedFunctionResourcesParameters: {
	// An inline resource that contains code for a user-defined function (UDF).
	// Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
	// +kubebuilder:validation:Optional
	inlineCode?: null | string @go(InlineCode,*string)

	// A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
	// +kubebuilder:validation:Optional
	resourceUri?: null | string @go(ResourceURI,*string)
}

// JobSpec defines the desired state of Job
#JobSpec: {
	forProvider: #JobParameters @go(ForProvider)

	// THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored
	// unless the relevant Crossplane feature flag is enabled, and may be
	// changed or removed without notice.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	initProvider?: #JobInitParameters @go(InitProvider)
}

// JobStatus defines the observed state of Job.
#JobStatus: {
	atProvider?: #JobObservation @go(AtProvider)
}

// Job is the Schema for the Jobs API. Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:subresource:status
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,gcp}
#Job: {
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.jobId) || has(self.initProvider.jobId)",message="jobId is a required parameter"
	spec:    #JobSpec   @go(Spec)
	status?: #JobStatus @go(Status)
}

// JobList contains a list of Jobs
#JobList: {
	items: [...#Job] @go(Items,[]Job)
}
